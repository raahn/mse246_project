---
title: "MS&E 246 Final Report"
author: "Samuel Hansen, Theo Vadpey, Alex Elkrief, Ben Ertringer"
date: "2/23/2017"
output: 
  html_document:
    number_sections: yes
    toc: true
    keep_md: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, 
                      message = FALSE, cache = TRUE, eval = TRUE)
```

```{r}
# Initialize libraries 
library(ggrepel)
library(knitr)
library(lubridate)
library(caret)
library(stringr)
library(scales)
library(plotROC)
library(pROC)
library(tidyverse)

# Initialize input files 
train_file_in <- "../data/train.rds"
test_file_in <- "../data/test.rds"
eda_file_in <- "../data/SBA_Loan_data_1.csv"

# Read in data for EDA
eda_data <- 
  read_csv(eda_file_in) %>%
  plyr::rename(replace = c("2DigitNAICS" = "NAICS")) %>%
  mutate(ApprovalDate = mdy(ApprovalDate))

# Read in train and test data 
train <- read_rds(train_file_in)
test <- read_rds(test_file_in)
```

#Exectutive Summary

In *MS&E 246: Financial Risk Analytics*, our team analyzed a data set of 
roughly 150,000 loans backed by the US Small Business Administration 
(SBA) between 1990 and 2014. In doing so, we aimed to implement and test models
of the risk and loss of loan default. This report summarizes our findings from exploratory data analysis, details our approaches to modeling loan 
default probability and loss, and presents our methods of estimating
the loss distributions of tranches backed by a 500-loan portfolio. 

#Exploratory Data Analysis

Prior to model building, we explored the data to detect patterns that may 
provide signal for models of loan default. Because we first aimed to build 
binary response models of default probability, we excluded "Exempt" loans from 
our exploratory analysis. Subsequently, we examined the relationship between 
default rates and the predictor variables, including `Business Type`, 
`Loan Amount`, `NAICS Code`, and `Loan Term`, among others. 

Further, we collected additional predictor variables such as monthly 
`GDP`, `Crime Rate`, and `Unemployment Rate` by State, as well as macroeconomic
predictors such as monthly measures of the `S&P 500`, `Consumer Price Index`, 
and 14 other volatility market indices (see Data Cleaning section for 
data collection details). We include insights from exploratory analysis of 
these measures as well. 

##Default Rate vs. Business Type 

First, we examined the relationship between default rate and `Business Type`
by loan approval year. As shown on the plot below, we observe an interaction
effect between these three features, such that default rates spiked for 
loans that were approved around the Great Recession (approximately 2006- 2009). 
Further, the different trajectories of the 3 curves implies the "individual" 
`Business Type` suffered greater default rates than corporations and 
partnerships. Although corporations constitute a greater share of the data set,
as evidenced by the greater mass in the red circles, they exhibit medium 
default risk, as compared to the other business types. Taken together, 
this plot reveals business types were affected differently by the recession,
offering useful signal for subsequent modeling. 

```{r}
eda_data %>%
  group_by(BusinessType, year = year(ApprovalDate)) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = frac_defaulted, color = BusinessType)) +
  geom_point(mapping = aes(size = count)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(name = "Number of Loans",
                        labels = comma) +
  scale_color_discrete(name = "Business Type") + 
  labs(x = "Approval Year", y = "Default Rate",
       title = "Default Rate vs. Business Type by Approval Year")
```

##Default Rate by Loan Amount

Second, we examined whether we would observe a similar time-dependent 
interaction effect between default rate and `Loan Amount`. The plot below 
reveals that loans of all sizes approved around the Great Recession faced the
greatest default rates. However, loans of sizes \$500k-\$1m and \$1m-\$2m
appear to have experienced larger default rates over time compared to smaller
loans of size \$100k-\$300k and \$300k-\$500k. The spiking behavior of \$1m-\$2m
loan in 1999 and of loans greater than \$2m seem to be due to small sample 
sizes, as depicted by circle diameter. Overall, since loans of different sizes
have different default rate patterns over time, we would also expect 
the `Loan Amount` feature to offer predictive power. 

```{r}
loan_labels <- c("(-10,1e+05]" = "< 100k",
                 "(1e+05,3e+05]" = "100k - 300k",
                 "(3e+05,5e+05]" = "300k - 500k",
                 "(5e+05,1e+06]" = "500k - 1m",
                 "(1e+06,2e+06]" = "1m - 2m",
                 "(2e+06,4e+06]" = "> 2m")
eda_data %>%
  mutate(loan_bin = cut(GrossApproval, 
                        breaks = c(-10, 100000, 300000, 500000, 
                                   1000000, 2000000, 4000000)),
         loan_bin = plyr::revalue(loan_bin, loan_labels),
         year = year(ApprovalDate)) %>%
  group_by(loan_bin, year) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = frac_defaulted, color = loan_bin)) +
  geom_point(mapping = aes(size = count)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(name = "Number of Loans",
                        labels = comma) + 
  scale_color_discrete(name = "Loan Amount ($)") + 
  labs(x = "Approval Year", 
       y = "Default Rate", 
       title = "Default Rate by Loan Amount")
```

##Default Rate by NAICS Code

Third, we hypothesized different economic sectors would exhibit 
different default rates over time. In turn, we extracted the North American 
Industry Classification System (NAICS) code for each loan and truncated it
to the first two digits, which represents broad industry classes such as 
"Agriculture" and "Manufacturing." The following plot shows the default 
rate for loans of each truncated NAICS code approved in each year between 
1990-2014. We observe considerable variance in default rates between sectors;
for instance, codes 72, corresponding to "Accommodation & Food Services", 
has one of the highest default rates even before the recession. However,
code 54, corresponding to "Professional, Scientific, and Technical Services,"
consistently has one the lowest default rates. These patterns are consistent
with intuition, and underscore the value of including the truncated NAICS code
as a predictive feature of defaulting. 

```{r}
eda_data %>%
  filter(NAICS != "00",
         NAICS != "99") %>%
  group_by(NAICS, year = year(ApprovalDate)) %>%
  summarise(default_rate = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = default_rate, color = NAICS)) +
  geom_point() +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Approval Year", y = "Default Rate", title = "Default Rate by NAICS Code")
```

##State GDP vs. Default Rate

- Make plot here

#Modeling Default Probability 

Building upon our exploratory data analysis, we constructed two types of 
predictive models of loan default probability: binary response models and 
cox proportional hazards models. Here, we present our approach to fitting
both model types, including data cleaning, feature engineering, 
feature selection, hyper-parameter optimization, and evaluation. 

##Binary Response Models

First, we built binary response models of small-businesses defaulting on loans,
which estimate the probability that a given loan *ever* defaults. To do so,
we implemented a machine learning pipeline that: 

1. Performs feature engineering;
2. Splits the data into train and test sets;
3. Normalizes continuous features;
4. Selects features using recursive feature elimination;
5. Trains binary response predictive models, including LASSO and random 
forests. 

Lastly, we evaluate the performance of these models on resampled partitions 
of the training data, and on a held-out test set in terms of AUC, sensitivity, 
and calibration. 

###Feature Engineering

Building on insights derived from exploratory data analysis,
we engineered the following features from the raw data: 

- `NAICS_code`: truncated to the first two digits of the NAICS code;
- `subprogram`: condensed infrequent factor levels into "other" category;
- `approval_year`: extracted year from loan approval date-time object.
- `SameLendingState`: created flag for whether borrower received loan from in-state; 
- `MultiTimeBorrower`: created flag for whether loan recipient is multi-time borrower;
- `ThirdPartyLender` created flag for whether borrower received third party aide. 

In effect, these features represent dimensionality reduction of factors 
with many levels. For instance, there are 1,239 unique NAICS six-digit NAICS
codes in the raw data, yet only 25 unique 2-digit codes. Although we lose 
fine-grained detail by truncating the NAICS code, we aimed to optimize our
models by reducing variance introduced by high dimensionality. After applying
such dimension reductions, we eliminated extraneous variables, such as the 
Borrower's Zip Code and the Project's State, where were used to engineer
features. 

In addition to constructing features from the raw data, we also incorporated 
data from external sources, including monthly State-based measures of 
crime rate, GDP, and unemployment rate. We also joined in time-varying risk 
factors, including monthly snapshots of the `S&P 500`, `Consumer Price Index`, 
and 14 other volatility market indices. 

- BEN: Fill in where the data came from and any other important info 

###Data Splitting 

We randomly partitioned the data into 70% training and 30% test sets. 
This approach does not implement a time-based split, but rather a random 
sampling of observations over the entire 1990-2014 window. We adopted this 
splitting approach because we were interested in capturing the signal 
of the Great Recession within our models. Further, we did not create a 
validation set because we performed feature selection and hyper-parameter
optimization using cross-validation on the training set. 

###Data Preprocessing

After engineering features and joining in external data sources, 
we applied several preprocessing steps to our main data frame.
First, we centered and scaled the continuous predictors to apply regularization 
techniques during the modeling phase. Doing so adjusted for variables being
on different scales; for example, `Gross Approval` varies in dollar amounts 
from \$30,000  to \$4,000,000, whereas `Term in Months` ranges from 1 to 389. 
Second, we applied a filter to remove features with near zero variance to 
eliminate predictors that do not offer meaningful signal. 

```{r}
# Define unnecessary videos 
vars_to_drop <- c("GrossChargeOffAmount", "ChargeOffDate", 
                             "ApprovalDate", "first_zip_digit")

# Remove unnecesary features for modeling 
train <- train %>% select(-one_of(vars_to_drop))
test <- test %>% select(-one_of(vars_to_drop))
```

```{r}
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale", "nzv")

# Apply same pre-processing steps to the test set
preProcessObject <- preProcess(train, method = preProcessSteps)
train <- predict(preProcessObject, train)
test <- predict(preProcessObject, test)
```

###Feature Selection

To perform feature selection, we used recursive feature elimination
with 10-fold cross-validation. This method uses random forests to iteratively
remove variables with low variable importance, as measured by mean increase 
in out-of-bag area-under-the-curve (AUC). In other words, variables that 
do not contribute to significant improvements in AUC are eliminated. We
performed a grid search over the number of potential features to determine 
how many features to include. Note that factors were converted to separate dummy 
variables using a one-hot encoder. 

```{r, echo = FALSE}
rfe.results <- read_rds("../models/rfe.results.rds")
```

The following plot shows that recursive feature selection 
chose `r rfe.results[["results"]]$Variables[which.max(rfe.results[["results"]]$ROC)]` 
variables because AUC is maximized (see plot below). In effect, all variables
were kept because they offered predictive power regarding loan defaults. 

```{r}
ggplot(rfe.results) +
  labs(x = "Number of Variables",
       y = "AUC (Cross-Validated)",
       title = "Recursive Feature Elimination\nNumber of Variables vs. AUC")
```

The importances of the top 10 selected features are shown in the plot below.
We observe that State GDP, a monthly time-dependent risk factor, is the most 
important feature, meaning it led to the greatest average increase in AUC
across cross-validation iterations. State unemployment rate and crime rate 
are also highly important, suggesting local time-dependent risk factors 
are the most predictive of whether a loan defaults. 

The importance of NAICS code 72, corresponding to 
"Accommodation & Food Services", is consistent with our exploratory 
data analysis finding that the sector is especially risk prone. Borrower States
such as Michigan, California,and Florida also offer predictive power of 
defaulting. Lastly, the importances of the Collar Index (CLL) and Iron 
Butterfly Index (BFLY) imply market volatility measures also improve 
the discrimination loan defaults. 

```{r}
data_frame(predictor = rownames(varImp(rfe.results)), 
           var_imp = varImp(rfe.results)$Overall) %>%
  slice(1:10) %>%
  ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "", 
       y = "Variable Importance", 
       title = "Recursive Feature Elimination Variable Importance")
```

###Model Fitting 

Using these selected features, we fit models predicting the binary outcome
of whether a small business defaults on a loan. We constructed linear and 
nonlinear models, including a logistic regression model with the elastic net 
penalty, a random forest classifier, and a gradient boosting machine 
classifier. To tune hyper-parameters, we used 10-fold cross-validation with the 
one standard-error rule, which selects parameters that obtain the highest 
cross-validated AUC within one standard error of the maximum. For each model
type, we performed a grid search over the hyper-parameters to ensure optimal
selection.

####Logistic Regression with Elastic Net

AUC was used to select the optimal elastic net model using the one SE rule.
The final values used for the model were `alpha` = 0.1 and `lambda` = 0.

```{r, echo = FALSE, fig.width=6, fig.height=6}
elastic.fit <- read_rds("../models/elastic.fit.rds")
plot(elastic.fit)
```

####Random Forest Classifier

AUC was used to select the optimal random forest model using the one SE rule.
The final value used for the model was `mtry` = 8.

```{r, echo = FALSE, fig.width=6, fig.height=6}
rf.fit <- read_rds("../models/rf.fit.rds")
plot(rf.fit)
```

####Gradient Boosting Machine Classifier

AUC was used to select the optimal extreme gradient boosting model using 
the one SE rule. The final values used for the model were `nrounds` = 100, 
`max_depth` = 6, `eta` = 0.03, `gamma` = 0, `colsample_bytree` = 0.4, 
`min_child_weight` = 1 and `subsample` = 0.5. 

```{r, echo = FALSE, fig.width=6, fig.height=6}
xgb.fit <- read_rds("../models/xgb.fit.rds")
plot(xgb.fit)
```

###Model Evaluation 

####In-Sample Evaluation 

#####Training AUC and Sensitivity of Best Models 

The following plot compares averaged **training** area under the ROC curve 
and sensitivity across the model types with optimized parameters.

```{r}
# Evaluate performance of each model on training data
bind_rows(getTrainPerf(elastic.fit), 
          getTrainPerf(rf.fit),
          getTrainPerf(xgb.fit)) %>% 
  as_data_frame() %>%
  mutate(method = recode(method, 
                         "glmnet" = "Elastic Net", 
                         "rf" = "Random Forest",
                         "xgbTree" = "Gradient Boosting")) %>%
  ggplot(mapping = aes(x = TrainSens, y = TrainROC, label = method)) +
  geom_point() +
  geom_text_repel() +
  labs(x = "Train Sensitivity", y = "Train AUC", 
       title = "Training Sensitivity vs. AUC by Model Type")
```

#####Distribution of Resampled Training AUC, Sensitivity, and Specificity 

To examine the spread of **training** area under the ROC curve,
sensitivity, and specificity across model types, we leverage the resampled
data generated during the cross-valiation of modeling fitting to plot 
their respective distributions.  

```{r, fig.width=9, fig.height=6}
# Generate resamples from data 
resamps <- resamples(list(Elastic_Net = elastic.fit,
                          Random_Forest = rf.fit,
                          Gradient_Boosting = xgb.fit))

# Generate boxplots 
metric_labs = c("ROC" = "AUC", "Sens" = "Sensitivity", "Spec" = "Specificity")
resamps$values %>%
  gather(method, value, `Elastic_Net~ROC`:`Gradient_Boosting~Spec`) %>%
  separate(method, c("method", "metric"), sep = "~", remove = TRUE) %>%
  ggplot(mapping = aes(x = method, y = value)) +
  geom_boxplot() +
  facet_wrap(~metric, scales = "free_y", 
             labeller = labeller(metric = metric_labs)) +
  labs(x = "Model Type", y = "Metric Value", 
       title = "Spread of Training AUC, Sensitivity, and Specificity") +
  theme(axis.text.x=element_text(angle = 45, hjust = 1))

```

#####Training ROC Curves 

Lastly, we can examine the training ROC curves by model type. 

```{r, fig.width=9, fig.height=6}
# Evaluate performance of trained models on training set 
trainResults <- data.frame(true_value = train$LoanStatus)
trainResults$randomForest <- predict(rf.fit, train, type = "prob")[,"default"]
trainResults$elasticNet <- predict(elastic.fit, train, type = "prob")[,"default"]
trainResults$gradientBoosting <- predict(xgb.fit, train, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = trainResults$randomForest,
                       response = trainResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = trainResults$elasticNet,
                       response = trainResults$true_value)),
             gradientBoosting = pROC::auc(roc(predictor = trainResults$gradientBoosting,
                       response = trainResults$true_value))) %>%
  gather(method, auc_value, randomForest:gradientBoosting) %>%
  mutate(auc_label = paste("AUC =", round(auc_value, 3)))

# Gather results in long format 
trainResults <- 
  trainResults %>%
  gather(method, predicted_prob, randomForest:gradientBoosting) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))

# Plot ROC curves by model type 
model_labels <- c("randomForest" = "Random Forest",
                  "elasticNet" = "Elastic Net",
                  "gradientBoosting" = "Gradient Boosting")
trainResults %>%
  ggplot(mapping = aes(d = true_value, m = predicted_prob)) +
  geom_roc(n.cuts = 5, labelsize = 2, labelround = 3) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  labs(x = "False Positive Fraction", y = "True Positive Fraction",
       title = "ROC Curves by Model Type") +
  facet_wrap(~method, labeller = labeller(method = model_labels)) +
  geom_text(data = aucs, aes(x = 0.75, y = 0.5, label = auc_label), 
                    colour = "black", inherit.aes = FALSE, parse = FALSE)
```

####Out-of-Sample Evaluation 

#####Test ROC Curves 

```{r}
# Evaluate performance of trained models on test set 
testResults <- data.frame(true_value = test$LoanStatus)
testResults$randomForest <- predict(rf.fit, test, type = "prob")[,"default"]
testResults$elasticNet <- predict(elastic.fit, test, type = "prob")[,"default"]
testResults$gradientBoosting <- predict(xgb.fit, test, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = testResults$randomForest,
                       response = testResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = testResults$elasticNet,
                       response = testResults$true_value)),
             gradientBoosting = pROC::auc(roc(predictor = testResults$gradientBoosting,
                       response = testResults$true_value))) %>%
  gather(method, auc_value, randomForest:gradientBoosting) %>%
  mutate(auc_label = paste("AUC =", round(auc_value, 3)))

# Gather results in long format 
testResults <- 
  testResults %>%
  gather(method, predicted_prob, randomForest:gradientBoosting) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))
```

```{r, fig.width=9, fig.height=6}
# Plot ROC curves by model type 
model_labels <- c("randomForest" = "Random Forest",
                  "elasticNet" = "Elastic Net",
                  "gradientBoosting" = "Gradient Boosting")
testResults %>%
  ggplot(mapping = aes(d = true_value, m = predicted_prob)) +
  geom_roc(n.cuts = 5, labelsize = 2, labelround = 3) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  labs(x = "False Positive Fraction", y = "True Positive Fraction",
       title = "ROC Curves by Model Type") +
  facet_wrap(~method, labeller = labeller(method = model_labels)) +
  geom_text(data = aucs, aes(x = 0.75, y = 0.5, label = auc_label), 
                    colour = "black", inherit.aes = FALSE, parse = FALSE)
```

#####Test Calibration Plots

The following calibration plots depict the extent to which our models' 
predicted probabilities of default align with the actual probabilities of
default. 

```{r, fig.width=9, fig.height=6, eval = TRUE}
# Make calibration plots, facetted by model type
pred_prob_midpoints <- data_frame(midpoint = c(0, 0, rep(seq(0.05, 0.875, 0.05), each = 3), 0.9, 0.95, 1))

testResults %>%
  mutate(prob_bin = cut_width(predicted_prob, width = 0.05)) %>%
  group_by(prob_bin, method) %>%
  dplyr::summarise(prob_default = mean(true_value, na.rm = TRUE),
            n = n()) %>%
  bind_cols(., pred_prob_midpoints) %>%
  ungroup() %>%
  ggplot(mapping = aes(x = midpoint, y = prob_default)) +
  geom_line() +
  geom_point(mapping = aes(size = n)) +
  # geom_text_repel(mapping = aes(color = "red")) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  scale_x_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_y_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_colour_discrete(guide = FALSE) +
  scale_size(name = "Number of\nPredictions",
             labels = scales::comma) +
  labs(x = "Predicted Default Probability (Bin Midpoint)",
       y = "Observed Default Fraction",
       title = "Calibration Plot: Predicted vs. Observed Default Probability") +
  facet_wrap(~method, labeller = labeller(method = model_labels))
```

##Cox Proportional Hazards Models 

- THEO

#Modeling Loss at Default 

##Value-at-Risk

- ALEX

##Average Value-at-Risk

- ALEX

#Loss Distributions by Tranche

- BEN