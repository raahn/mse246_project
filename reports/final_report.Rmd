---
title: "MS&E 246 Final Report"
author: "Samuel Hansen, Theo Vadpey, Alex Elkrief, Ben Ertringer"
date: "2/23/2017"
output:
  html_document:
    keep_md: yes
    number_sections: yes
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, 
                      message = FALSE, cache = TRUE, eval = TRUE)
```

```{r}
# Initialize libraries 
library(ggrepel)
library(knitr)
library(lubridate)
library(caret)
library(stringr)
library(scales)
library(plotROC)
library(pROC)
library(tidyverse)

# Initialize input files 
train_file_in <- "../data/train.rds"
test_file_in <- "../data/test.rds"
full_file_in <- "../data/merged.rds"
eda_file_in <- "../data/SBA_Loan_data_1.csv"

# Read in data for EDA
eda_data <- 
  read_csv(eda_file_in) %>%
  plyr::rename(replace = c("2DigitNAICS" = "NAICS")) %>%
  mutate(ApprovalDate = mdy(ApprovalDate))

# Read in data for EDA with external variables 
full_data <-
  read_rds(full_file_in)

# Read in train and test data 
train <- read_rds(train_file_in)
test <- read_rds(test_file_in)
```

#Exectutive Summary

In *MS&E 246: Financial Risk Analytics*, our team analyzed a data set of 
roughly 150,000 loans backed by the US Small Business Administration 
(SBA) between 1990 and 2014. In doing so, we aimed to implement and test models
of the risk and loss of loan default. This report summarizes our findings from 
exploratory data analysis, details our approaches to modeling loan 
default probability and loss, and presents our methods of estimating
the loss distributions of tranches backed by a portfolio of loans. 

#Exploratory Data Analysis

Prior to model building, we explored the data to detect patterns that may 
provide signal for models of loan default. Because we first aimed to build 
binary response models of default probability, we excluded "Exempt" loans from 
our exploratory analysis. Subsequently, we examined the relationship between 
default rates and the predictor variables, including `Business Type`, 
`Loan Amount`, `NAICS Code`, and `Loan Term`, among others. 

Further, we collected additional predictor variables such as monthly 
`GDP`, `Crime Rate`, and `Unemployment Rate` by State, as well as macroeconomic
predictors such as monthly measures of the `S&P 500`, `Consumer Price Index`, 
and 14 other volatility market indices (see "Data Cleaning" section for 
data collection details). We include insights from exploratory analysis of 
these measures as well. 

##Default Rate vs. Business Type 

First, we examined the relationship between default rate and `Business Type`
by loan approval year. As shown on the plot below, we observe an interaction
effect between these three features, such that default rates spiked for 
loans that were approved around the Great Recession (approximately 2006- 2009). 
Further, the different trajectories of the 3 curves implies the "Individual" 
`Business Type` suffered greater default rates than corporations and 
partnerships. Although corporations constitute a greater share of the data set,
as evidenced by the greater mass in the red circles, they exhibit medium 
default risk, as compared to the other business types. Taken together, 
this plot reveals business types were affected differently by the recession,
offering useful signal for subsequent modeling. 

```{r}
eda_data %>%
  group_by(BusinessType, year = year(ApprovalDate)) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = frac_defaulted, color = BusinessType)) +
  geom_point(mapping = aes(size = count)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(name = "Number of Loans",
                        labels = comma) +
  scale_color_discrete(name = "Business Type") + 
  labs(x = "Approval Year", y = "Default Rate",
       title = "Default Rate vs. Business Type by Approval Year")
```

##Default Rate by Loan Amount

Second, we examined whether we would observe a similar time-dependent 
interaction effect between default rate and `Loan Amount`. The plot below 
reveals that loans of all sizes approved around the Great Recession faced the
greatest default rates. However, loans of sizes \$500k-\$1m and \$1m-\$2m
appear to have experienced larger default rates over time compared to smaller
loans of size \$100k-\$300k and \$300k-\$500k. The spiking behavior of \$1m-\$2m
loans in 1999 and of loans greater than \$2m seem to be due to small sample 
sizes, as depicted by circle diameter. Overall, since loans of different sizes
have different default rate patterns over time, we would also expect 
the `Loan Amount` feature to offer predictive power. 

```{r}
loan_labels <- c("(-10,1e+05]" = "< 100k",
                 "(1e+05,3e+05]" = "100k - 300k",
                 "(3e+05,5e+05]" = "300k - 500k",
                 "(5e+05,1e+06]" = "500k - 1m",
                 "(1e+06,2e+06]" = "1m - 2m",
                 "(2e+06,4e+06]" = "> 2m")
eda_data %>%
  mutate(loan_bin = cut(GrossApproval, 
                        breaks = c(-10, 100000, 300000, 500000, 
                                   1000000, 2000000, 4000000)),
         loan_bin = plyr::revalue(loan_bin, loan_labels),
         year = year(ApprovalDate)) %>%
  group_by(loan_bin, year) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = frac_defaulted, color = loan_bin)) +
  geom_point(mapping = aes(size = count)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(name = "Number of Loans",
                        labels = comma) + 
  scale_color_discrete(name = "Loan Amount ($)") + 
  labs(x = "Approval Year", 
       y = "Default Rate", 
       title = "Default Rate by Loan Amount")
```

##Default Rate by NAICS Code

Third, we hypothesized different economic sectors would exhibit 
different default rates over time. In turn, we extracted the North American 
Industry Classification System (NAICS) code for each loan and truncated it
to the first two digits, which represents broad industry classes such as 
"Agriculture" and "Manufacturing." The following plot shows the default 
rate for loans of each truncated NAICS code approved in each year between 
1990-2014. We observe considerable variance in default rates between sectors;
for instance, code 72, corresponding to "Accommodation & Food Services", 
has one of the highest default rates even before the recession. However,
code 54, corresponding to "Professional, Scientific, and Technical Services,"
consistently has one the lowest default rates. These patterns are consistent
with intuition, and underscore the value of including the truncated NAICS code
as a predictive feature of defaulting. 

```{r}
eda_data %>%
  filter(NAICS != "00",
         NAICS != "99") %>%
  group_by(NAICS, year = year(ApprovalDate)) %>%
  summarise(default_rate = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = default_rate, color = NAICS)) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "NAICS Code") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Approval Year", y = "Default Rate", title = "Default Rate by NAICS Code")
```

##Default Rate by Subprogram Type

Fourth, we compared the default rates between different loan subprogram types. 
The plot below shows the default rates of the different loan
subprograms versus their respective counts in the data. We observe 
that the PSF subprogram is the most common and has medium default risk. 
However, loans in the Premier Certified Lenders Program (PCLP) are less 
common, but have higher default risk. This suggests `Subprogram Type` offers 
useful signal for predicting default risk. Lastly, the loans belonging to the 
Delta and Refinance subprograms are highly uncommon and have low default risk. 
In order to reduce to the dimensionality of the feature space, we collapsed 
these two factor levels into "Other."

```{r}
eda_data %>%
  group_by(subpgmdesc) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = count, y = frac_defaulted, color = subpgmdesc)) +
  geom_point() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::comma) +
  scale_color_discrete(name = "Subprogram Type") + 
  labs(x = "Count of Subprogram", y = "Default Rate",
       title = "Default Rate vs. Subprogram Type")
```

##State GDP vs. Default Rate

- Make plot here

#Modeling Default Probability 

Building upon our exploratory data analysis, we constructed two types of 
predictive models of loan default probability: binary response models and 
the Cox Proportional Hazards model. Here, we present our approach to fitting
both model types, including data cleaning, feature engineering, 
feature selection, hyper-parameter optimization, and evaluation. 

##Binary Response Models

First, we built binary response models of small-businesses defaulting on loans,
which estimate the probability that a given loan *ever* defaults. To do so,
we implemented a machine learning pipeline that: 

1. Performs feature engineering;
2. Splits the data into train and test sets;
3. Normalizes continuous features;
4. Selects features using recursive feature elimination;
5. Trains binary response predictive models. 

Lastly, we evaluated the performance of these models on resampled partitions 
of the training data, and on a held-out test set in terms of AUC, sensitivity, 
and calibration. 

###Feature Engineering

Building on insights derived from exploratory data analysis,
we engineered the following features from the raw data: 

- `NAICS_code`: truncated to the first two digits of the NAICS code;
- `subprogram`: condensed infrequent factor levels into "other" category;
- `approval_year`: extracted year from loan approval date-time object.
- `SameLendingState`: created flag for whether borrower received loan from in-state; 
- `MultiTimeBorrower`: created flag for whether loan recipient is a multi-time borrower;
- `ThirdPartyLender` created flag for whether borrower received third party aide. 

In effect, these features represent dimensionality reduction of factors 
with many levels. For instance, there are 1,239 unique NAICS six-digit NAICS
codes in the raw data, yet only 25 unique 2-digit codes. Although we lose 
fine-grained detail by truncating the NAICS code, we aimed to optimize our
models by reducing variance introduced by high dimensionality. After applying
such dimension reductions, we eliminated extraneous variables, such as the 
Borrower's Zip Code and the Project's State, which were used to engineer
features. 

In addition to constructing features from the raw data, we also incorporated 
data from external sources, including monthly State-based measures of 
crime rate, GDP, and unemployment rate. We also joined in time-varying risk 
factors, including monthly snapshots of the `S&P 500`, `Consumer Price Index`, 
and 14 other volatility market indices. 

- BEN: Fill in where the data came from and any other important info 

###Data Splitting 

We randomly partitioned the data into 70% training and 30% test sets. 
This approach does not implement a time-based split, but rather a random 
sampling of observations over the entire 1990-2014 window. We adopted this 
splitting approach because we were interested in capturing the signal 
of the Great Recession within our models. Further, we did not create a 
validation set because we performed feature selection and hyper-parameter
optimization using cross-validation on the training set. 

###Data Preprocessing

After engineering features and joining in external data sources, 
we applied several preprocessing steps to our main data frame.
First, we centered and scaled the continuous predictors to apply regularization 
techniques during the modeling phase. Doing so adjusted for variables being
on different scales; for example, `Gross Approval` varies in dollar amounts 
from \$30,000  to \$4,000,000, whereas `Term in Months` ranges from 1 to 389. 
Second, we applied a filter to remove features with near zero variance to 
eliminate predictors that do not offer meaningful signal. 

```{r}
# Define unnecessary videos 
vars_to_drop <- c("GrossChargeOffAmount", "ChargeOffDate", 
                             "ApprovalDate", "first_zip_digit")

# Remove unnecesary features for modeling 
train <- train %>% select(-one_of(vars_to_drop))
test <- test %>% select(-one_of(vars_to_drop))
```

```{r}
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale", "nzv")

# Apply same pre-processing steps to the test set
preProcessObject <- preProcess(train, method = preProcessSteps)
train <- predict(preProcessObject, train)
test <- predict(preProcessObject, test)
```

###Feature Selection

To perform feature selection, we used recursive feature elimination
with 10-fold cross-validation. This method uses random forests to iteratively
remove variables with low variable importance, as measured by mean increase 
in out-of-bag area-under-the-curve (AUC). In other words, variables that 
do not contribute to significant improvements in AUC are eliminated. We
performed a grid search over the number of potential features to determine 
how many features to include. Note that factors were converted to separate dummy 
variables using a one-hot encoder. 

```{r, echo = FALSE}
rfe.results <- read_rds("../models/rfe.results.rds")
```

The following plot shows that recursive feature selection 
chose `r rfe.results[["results"]]$Variables[which.max(rfe.results[["results"]]$ROC)]` 
variables because AUC is maximized (see plot below). In effect, all variables
were kept because they offered predictive power regarding loan defaults. 

```{r}
ggplot(rfe.results) +
  labs(x = "Number of Variables",
       y = "AUC (Cross-Validated)",
       title = "Recursive Feature Elimination\nNumber of Variables vs. AUC")
```

The importances of the top 10 selected features are shown in the plot below.
We observe that State GDP, a monthly time-dependent risk factor, is the most 
important feature, meaning it led to the greatest average increase in AUC
across cross-validation iterations. State unemployment rate and crime rate 
are also highly important, suggesting local time-dependent risk factors 
are the most predictive of whether a loan defaults. 

The importance of NAICS code 72, corresponding to 
"Accommodation & Food Services", is consistent with our exploratory 
data analysis finding that the sector is especially risk prone. Borrower States
such as Michigan, California, and Florida also offer predictive power regarding 
defaulting. Lastly, the importances of the Collar Index (CLL) and Iron 
Butterfly Index (BFLY) imply market volatility measures also improve 
the discrimination of loan defaults. 

```{r}
data_frame(predictor = rownames(varImp(rfe.results)), 
           var_imp = varImp(rfe.results)$Overall) %>%
  slice(1:10) %>%
  ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "", 
       y = "Variable Importance", 
       title = "Recursive Feature Elimination Variable Importance")
```

###Model Fitting 

Using these selected features, we fit models predicting the binary outcome
of whether a small business defaults on a loan. We constructed linear and 
nonlinear models, including a logistic regression model with the elastic net 
penalty, a random forest classifier, and a gradient boosting machine 
classifier. To tune hyper-parameters, we used 10-fold cross-validation with the 
one standard-error rule, which selects parameters that obtain the highest 
cross-validated AUC within one standard error of the maximum. For each model
type, we performed a grid search over the hyper-parameters to ensure optimal
selection.

####Logistic Regression with Elastic Net

First, AUC was used to select the optimal logistic regression model with an
elastic net penalty using the one standard-error rule. As shown in the plot 
below, the final values used for the model were `alpha` = 0.1 and `lambda` = 0,
indicated by the spike in the red curve at `alpha` = 0.1. This implies the 
optimal model used the ridge penalty more than the LASSO penalty with minimal 
regularization. 

```{r, echo = FALSE}
elastic.fit <- read_rds("../models/elastic.fit.rds")
elastic.fit$results %>%
  ggplot(mapping = aes(x = alpha, y = ROC, color = factor(lambda))) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "Lambda Penalty") +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  scale_y_continuous(breaks = seq(0.8, 0.83, 0.005)) +
  labs(x = "Mixing Percentage (Alpha)",
       y = "AUC (Cross-Validated)",
       title = "Logistic Regression Hyper-Parameter Optimization")
```

####Random Forest Classifier

Second, AUC was used to select the optimal random forest model, which selected 
`mtry` = 8 as the best parameter. This means 8 random predictors were chosen 
to build each tree of the random forest. The plot below shows steadily declining
AUC as the number of randomly chosen predictors increases, indicating that
the optimal model is sparsest. 

```{r, echo = FALSE}
rf.fit <- read_rds("../models/rf.fit.rds")
rf.fit$results %>%
  ggplot(mapping = aes(x = mtry, y = ROC)) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "Lambda Penalty") +
  labs(x = "Number of Randomly Selected Predictors",
       y = "AUC (Cross-Validated)",
       title = "Random Forest Hyper-Parameter Optimization")
```

####Gradient Boosting Machine Classifier

Third, AUC was similarly used to select the optimal gradient boosting machine 
(GBM) model. The final values used for the model were `nrounds` = 100, 
`max_depth` = 6, `eta` = 0.03, `gamma` = 0, `colsample_bytree` = 0.4, 
`min_child_weight` = 1 and `subsample` = 0.5. This means that the tuning 
procedure utilized a learning rate of 0.03 and a minimum loss reduction of 0, 
resulting in the optimal model with 100 trees of maximum depth 6 that subsamples
50% of the observations and 40% of the features for each tree. This combination
of optimal hyper-parameters is shown by the spike of the red curve in the first 
subplot at the maximum tree depth of 6.  

```{r, echo = FALSE}
xgb.fit <- read_rds("../models/xgb.fit.rds")
subsamp_labs = c("0.5" = "Subsample = 50%",
                 "0.75" = "Subsample = 75%",
                 "1" = "Subsample = 100%")
xgb.fit$results %>%
  dmap_at("subsample", as.factor) %>%
  ggplot(mapping = aes(x = max_depth, y = ROC, color = factor(colsample_bytree))) +
  geom_point() +
  geom_line() +
  facet_grid(subsample~., labeller = labeller(subsample = subsamp_labs)) +
  scale_color_discrete(name = "Fraction of randomly\nselected features\nfor each tree") +
  labs(x = "Maximum Tree Depth", 
       y = "AUC (Cross-Validated)",
       title = "Gradient Boosting Hyper-Parameter Optimization")
```

Examining the variable importance of the final GBM model, we observe the most 
important feature for predicting defaults is the Collar Index (CLL), which is
"designed to provide investors with insights as to how one might protect an 
investment in S&P 500 stocks against steep market declines" ([CBOE](http://www.cboe.com/products/strategy-benchmark-indexes/collar-indexes/cboe-s-p-500-95-110-collar-index-cll)). Other important features include the national 
consumer price index (CPI), State GDP, crime, and unemployment rates, 
loan amount, and Chicago Board Options Exchange (CBOE) indices including the 
Butterfly Index (BFLY), the Iron Condor Index (CNDR), and the Volatility index 
(VIX). Such variables are "important" because they lead to the greatest 
improvements to cross-validated AUC across boosting iterations. 

```{r}
data_frame(predictor = rownames(varImp(xgb.fit)$importance), 
           var_imp = varImp(xgb.fit)$importance$Overall) %>%
  top_n(10) %>%
  ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "", 
       y = "Variable Importance", 
       title = "Gradient Boosting Machine Variable Importance")
```

###Model Evaluation 

After we optimized the hyper-parameters of our models, we evaluated the models  
using in-sample and out-of-sample metrics, including AUC, sensitivity,
ROC curves, and calibration. To do so, we used these "best" models to 
predict loan defaults in the training and test sets. 

####In-Sample Evaluation 

#####Training AUC and Sensitivity of Best Models 

The following plot compares averaged **training** area under the ROC curve 
and sensitivity across the model types with optimized parameters. We observe
that the gradient boosting machine classifier has the highest AUC and 
sensitivity, whereas the logistic regression model with the elastic net penalty
performs the worst. 

```{r}
# Evaluate performance of each model on training data
bind_rows(getTrainPerf(elastic.fit), 
          getTrainPerf(rf.fit),
          getTrainPerf(xgb.fit)) %>% 
  as_data_frame() %>%
  mutate(method = recode(method, 
                         "glmnet" = "Elastic Net", 
                         "rf" = "Random Forest",
                         "xgbTree" = "Gradient Boosting")) %>%
  ggplot(mapping = aes(x = TrainSens, y = TrainROC, label = method)) +
  geom_point() +
  geom_text_repel() +
  labs(x = "Train Sensitivity", y = "Training AUC", 
       title = "Training Sensitivity vs. AUC by Model Type")
```

#####Distribution of Resampled Training AUC, Sensitivity, and Specificity 

To examine the spread of **training** area under the ROC curve,
sensitivity, and specificity across model types, we leverage the resampled
data generated during the cross-validation of model fitting to plot 
their respective distributions. In the following plot, we observe that 
the GBM classifier has the highest median AUC, sensitivity, and specificity,
as well as the smallest spread. Although the random forest classifier has 
comparable sensitivity, it exhibits enormous variance compared to the other 
models, suggesting it is prone to overfitting. For this reason, the logistic
regression classifier (a linear model) outperforms the random forest classifier
(a non-linear model) in terms of AUC and specificity. 

```{r, fig.width=9, fig.height=6}
# Generate resamples from data 
resamps <- resamples(list(Elastic_Net = elastic.fit,
                          Random_Forest = rf.fit,
                          Gradient_Boosting = xgb.fit))

# Generate boxplots 
metric_labs = c("ROC" = "AUC", "Sens" = "Sensitivity", "Spec" = "Specificity")
resamps$values %>%
  gather(method, value, `Elastic_Net~ROC`:`Gradient_Boosting~Spec`) %>%
  separate(method, c("method", "metric"), sep = "~", remove = TRUE) %>%
  ggplot(mapping = aes(x = method, y = value)) +
  geom_boxplot() +
  facet_wrap(~metric, scales = "free_y", 
             labeller = labeller(metric = metric_labs)) +
  labs(x = "Model Type", y = "Metric Value", 
       title = "Spread of Training AUC, Sensitivity, and Specificity") +
  theme(axis.text.x=element_text(angle = 45, hjust = 1))

```

#####Training ROC Curves 

Lastly, we can examine the **training** ROC curves by model type. 
We observe that the random forest model has a near-perfect ROC curve,
which also implies it is overfitting to the training data. The GBM model again
performs worse than the random forest model on the training data, but likely 
because it is avoiding overfitting. The logistic regression model with the 
elastic net penalty performs the worst. 

```{r, fig.width=9, fig.height=6}
# Evaluate performance of trained models on training set 
trainResults <- data.frame(true_value = train$LoanStatus)
trainResults$randomForest <- predict(rf.fit, train, type = "prob")[,"default"]
trainResults$elasticNet <- predict(elastic.fit, train, type = "prob")[,"default"]
trainResults$gradientBoosting <- predict(xgb.fit, train, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = trainResults$randomForest,
                       response = trainResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = trainResults$elasticNet,
                       response = trainResults$true_value)),
             gradientBoosting = pROC::auc(roc(predictor = trainResults$gradientBoosting,
                       response = trainResults$true_value))) %>%
  gather(method, auc_value, randomForest:gradientBoosting) %>%
  mutate(auc_label = paste("AUC =", round(auc_value, 3)))

# Gather results in long format 
trainResults <- 
  trainResults %>%
  gather(method, predicted_prob, randomForest:gradientBoosting) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))

# Plot ROC curves by model type 
model_labels <- c("randomForest" = "Random Forest",
                  "elasticNet" = "Elastic Net",
                  "gradientBoosting" = "Gradient Boosting")
trainResults %>%
  ggplot(mapping = aes(d = true_value, m = predicted_prob)) +
  geom_roc(n.cuts = 5, labelsize = 2, labelround = 3) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  labs(x = "False Positive Fraction", y = "True Positive Fraction",
       title = "ROC Curves by Model Type") +
  facet_wrap(~method, labeller = labeller(method = model_labels)) +
  geom_text(data = aucs, aes(x = 0.75, y = 0.5, label = auc_label), 
                    colour = "black", inherit.aes = FALSE, parse = FALSE)
```

####Out-of-Sample Evaluation 

#####Test ROC Curves 

We evaluated our best models on a held-out test set representing 30% of the 
original data. The ROC curves below reveal that the GBM model performed 
the best on the test set, followed by the logistic regression model, and 
finally, the random forest classifier. The weak performance of the random forest 
classifier is likely due to overfitting on the training set. Nevertheless, 
all models achieve good performance over "random guessing" baselines. 

```{r}
# Evaluate performance of trained models on test set 
testResults <- data.frame(true_value = test$LoanStatus)
testResults$randomForest <- predict(rf.fit, test, type = "prob")[,"default"]
testResults$elasticNet <- predict(elastic.fit, test, type = "prob")[,"default"]
testResults$gradientBoosting <- predict(xgb.fit, test, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = testResults$randomForest,
                       response = testResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = testResults$elasticNet,
                       response = testResults$true_value)),
             gradientBoosting = pROC::auc(roc(predictor = testResults$gradientBoosting,
                       response = testResults$true_value))) %>%
  gather(method, auc_value, randomForest:gradientBoosting) %>%
  mutate(auc_label = paste("AUC =", round(auc_value, 3)))

# Gather results in long format 
testResults <- 
  testResults %>%
  gather(method, predicted_prob, randomForest:gradientBoosting) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))
```

```{r, fig.width=9, fig.height=6}
# Plot ROC curves by model type 
model_labels <- c("randomForest" = "Random Forest",
                  "elasticNet" = "Elastic Net",
                  "gradientBoosting" = "Gradient Boosting")
testResults %>%
  ggplot(mapping = aes(d = true_value, m = predicted_prob)) +
  geom_roc(n.cuts = 5, labelsize = 2, labelround = 3) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  labs(x = "False Positive Fraction", y = "True Positive Fraction",
       title = "ROC Curves by Model Type") +
  facet_wrap(~method, labeller = labeller(method = model_labels)) +
  geom_text(data = aucs, aes(x = 0.75, y = 0.5, label = auc_label), 
                    colour = "black", inherit.aes = FALSE, parse = FALSE)
```

#####Test Calibration Plots

Lastly, we evaluated the calibration of the our models' predicted probabilities
of loan default against the observed fraction of defaults in the data. 
A point on the dashed line means that the model's predicted probability
of default matched the empirical default rate. Points to the right of the line 
mean the model overestimated the default probability, whereas points to the 
left mean the model underestimated the default probability. 

The GBM model achieves the best calibration because its points follow 
the dashed line most closely. The logistic regression model with the elastic net
penalty achieves comparable performance; however, the random forest classifier
tends to overestimate default probabilities. Again, this weaker performance 
is likely due to overfitting. 

```{r, fig.width=10, fig.height=6}
# Make calibration plots, facetted by model type
pred_prob_midpoints <- data_frame(midpoint = c(0, 0, rep(seq(0.05, 0.875, 0.05), each = 3), 0.9, 0.95, 1))

testResults %>%
  mutate(prob_bin = cut_width(predicted_prob, width = 0.05)) %>%
  group_by(prob_bin, method) %>%
  dplyr::summarise(prob_default = mean(true_value, na.rm = TRUE),
            n = n()) %>%
  bind_cols(., pred_prob_midpoints) %>%
  ungroup() %>%
  ggplot(mapping = aes(x = midpoint, y = prob_default)) +
  geom_line() +
  geom_point(mapping = aes(size = n)) +
  # geom_text_repel(mapping = aes(color = "red")) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  scale_x_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_y_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_colour_discrete(guide = FALSE) +
  scale_size(name = "Number of\nPredictions",
             labels = scales::comma) +
  labs(x = "Predicted Default Probability (Bin Midpoint)",
       y = "Observed Default Fraction",
       title = "Calibration Plot: Predicted vs. Observed Default Probability") +
  facet_wrap(~method, labeller = labeller(method = model_labels))
```

The overfitting of the random forest classifier may be due to the fact that 
too many features were randomly selected to build trees at each
iteration. Our hyper-parameter optimization approach performed a grid search
over possible values of `mtry`, representing the number of features 
randomly chosen to build each tree in the forest. However, our grid may have
not been large enough, since the minimum value of `mtry` was chosen. 
However, computational resources limited our ability to refit models 
over a larger search space. 

Moreover, the gradient boosting machine classifier demonstrated the best
performance on the test set in terms of AUC and calibration.  

##Cox Proportional Hazards Model 

```{r load env, echo=F,results='hide',message=F}
load('../data/cox_data_environment_train.RData')
source('../scripts/cox_diagnostic_functions.R')
```
	
#### Motivation:
Survival analysis gives more detailed information about how the default risk of a loan varies over time. With binary classification, we estimated the probability that a given loan *ever* defaults. With a hazard model, we are able to estimate the probability that a loan defaults between any two points of time in its life.

#### Model Choice:
There exist many specialized Cox models that assume a particular form of the baseline hazard function. The Cox Proportional Hazards Model does not have this requirement. We can see this in the following description of the partial maximum likelihood procedure used to estimate the paramaters of the Cox PH model:

The form of the cox model is:
    $$h(t) = h_0(t)exp(\beta^T X)$$

Suppose there are $r$ observed death times in the data (all distinct), and that $t_j$ is a death time in the set of possible death times: $R = \{t_1,t_2,...,t_r\}$. 

Then the conditional probability that an individual dies at time $t_j$ given $t_j$ is a time of death in the set $R$:
     $$\frac{P(\text{individual with feature vector $X^{(j)}$ dies at $t_j$})}{P(\text{one death at $t_j$})}$$     
     $$ = \frac{ P(T = t_j | X^{(j)}, T \geq t_j ) }{P(T = t_j | X^{(k_0)}, T \geq t_j) \cup P(T = t_j | X^{(k_1)}, T \geq t_j) \cup ... P(T = t_j | X^{(k_q)}, T \geq t_j)}$$

Where $k_0, ..., k_q$ correspond to the indices of observations with event times greater than or equal to $t_j$. Since the probabilities in the denominator are *assumed to be conditionally independent*, the denominator can be expressed as a sum of probabilities. Converting the above to continuous time, we get: 

$$ = \frac{\text{lim}_{\delta \rightarrow 0}\frac{P(T < t_j + \delta | X^{(j)}, T \geq t_j )}{\delta}}{ \sum_{i = k_0}^{k_q} \text{lim}_{\delta \rightarrow 0}\frac{P(T < t_j + \delta | X^{(i)}, T \geq t_j)}{\delta} }$$

$$ = \frac{h_j(t_j)}{\sum_{i = k_0}^{k_q} h_i(t_j)} = \frac{h_0(t_j) exp(\beta^T X^{(j)})}{\sum_{i = k_0}^{k_q} h_0(t_j) exp(\beta^T X^{(i)})} = \frac{exp(\beta^T X^{(j)})}{\sum_{i = k_0}^{k_q} exp(\beta^T X^{(i)})}$$

And we can see that the contribution of any observation to the likelihood function will not be dependent on $h_0(t)$. $\square$
	      
#### Additional Modifications to the Data
Roughly 95% of loans in the training data set had a term of 20 years. We decided that considering loans with the same term was more appropriate for this analysis (84,949 loans). 

Within the training data, about 86\% of loans were right censored (term did not expire in window, and did not default), about 7\% of loans were paid off (term expired in window), and about 7% of loans defaulted within the window (figure 1). 
		
```{r distribution, eval=T,echo=F,fig.height=3.5,fig.width=3.5,fig.cap="Loans in training data by status"}
descrip_status = table(status_m)
names(descrip_status) = c("right_censored","paid_off","defaulted")
par(cex.axis=0.85,cex.lab=0.85,cex=0.85)
barplot(descrip_status/length(status_m),
        ylab='percent of training data')
```	

Polynomial terms up to *degree five* were added for all numeric variables. Our intention was to include these features to capture non-linearities in these variables, and conduct feature selection during model fitting (through regularization).

All numeric variables were centered to 0, and scaled by standard deviation.
	   
Missing values were set to 0 and an missing value indicator feature was added for each original variable.	   
      
Including expanded categorical variables, polynomials, and missing value dummies, the data had 201 features.
      
#### Kaplan-Meier Survival Curve
A Kaplan-Meier curve is a non-parametric estimate of the survival function, $S(t) = P(T>t)$, defined as: 

$$\hat{S(t)} = \prod_{t_i\leq t}\big[1 - \frac{d_i}{n_i}\big]$$

Where $\{t_1,...,t_r\}$ are the death times of observations in the data, $\{d_1,...,d_r\}$ are the number of deaths that occur at those times, and $\{n_1,...,n_r\}$ are the number of observations remaining in the at-risk population just before those times.

For expository purposes the following plots show the estimated survival function conditioned on select categorical variables such as a particular year, state, or status, as well as the general survival curve for our loan population. Note that the survival curve was significantly steeper for loans conditioned on these variables (a higher probability of default at all times).
	     
\begin{center}
\includegraphics[width=630pt]{../studies/surv_curvs.pdf}	
\end{center}	
		
#### Penalized Cox Proportional Hazards Model
For the purpose of feature selection, we fit a series of penalized Cox models to the training data.

We used an elastic net penalty-- a penalty term that is a linear combination of the $l_1$ and $l_2$ penalties.
   $$\lambda [ (1-\alpha)||\beta||_2 + \alpha||\beta||_1]$$
   
We fit models varying $\alpha$ and $\lambda$ in the penalty-- we selected the model with the largest evaluated value of the likelihood function.

\begin{center}      
\includegraphics[width=350pt,height=620pt]{../studies/heatmap.pdf}
\end{center}
	
The best model, in terms had a value of $\lambda$ very close to 0, and $\alpha$ very close to 0 (the ridge penalty). Ninety-seven variables of the original 201 had non-zero coefficients.
    
#### One Year and Five Year Prediction of default (out of sample)
The below figures show the out of sample performance of the one and five year probabilities estimated by the Cox model:   
\begin{center}   
\includegraphics[width=315pt,height=300pt]{../studies/p_1_roc_curve.pdf}
\includegraphics[width=315pt,height=300pt]{../studies/p_5_roc_curve.pdf}
\end{center}

# Portfolio Selection
For the next part of the project, we considered a portfolio of 500 loans selected from the withheld test data set. These loans met the following criteria:
    
1. Loans that had not defaulted as of 02-01-2010.
2. Loans that were approved before 02-01-2010.
3. Loans less than 15 years old.

These conditions were to ensure that the 500 loans in question were active as of the portfolio construction date, which we determined to be 02-01-2010. The 15 year age limit was so that estimation of 5 year ahead default probabilities would be valid.  
				
#Modeling Loss at Default 

##Value-at-Risk
#Data Preparation

##Cleaning

Before fitting the loss at default model, we clean the training set 
by filtering it to only include defaulted loans and by removing unnecessary
features such as `LoanStatus`. 

```{r}
# Prepare train data frame to only include defaulted loans 
cols_to_drop_train  = c("LoanStatus", "ChargeOffDate", "ApprovalDate", 
                  "first_zip_digit", "GrossChargeOffAmount", "GrossApproval")

train_with_defaults = 
  train %>%
  # Filter to only include defaulted loans
  filter(LoanStatus == "default") %>%
  # Make percentage loss column 
  mutate(percent_loss = GrossChargeOffAmount/GrossApproval) %>%
  # Filter out percent loss above 1
  filter(percent_loss <= 1) %>%
  # Remove unecessary variables 
  select(-one_of(cols_to_drop_train))

cols_to_drop_df  = c("LoanStatus", "ChargeOffDate", "ApprovalDate", 
                  "first_zip_digit", "GrossChargeOffAmount")
# Prepare portfolio data frame to match training variables 
portfolio = 
  portfolio %>%
  # Make percentage loss column 
  mutate(percent_loss = GrossChargeOffAmount/GrossApproval) %>%
  # Remove unecessary variables 
  select(-one_of(cols_to_drop_df)) 

# Convert probabilities object to data frame 
default_probs = 
  data_frame(one_year_prob = p_1 %>% unlist(),
             five_year_prob = p_5 %>% unlist()) 
```

##Pre-processing

We pre-process the data by centering and scaling all continuous features. 
We apply the same normalization from the training data with only defaulted loans
to the portfolio data used for prediction in the loss at default model. 
Similarly, we apply the same training normalization from the entire training 
set to the portfolio data used for prediction in the default probability model.

```{r}
# Define pre-processing steps
preProcessSteps = c("nzv")
```

#Feature Selection

To select the features that are used in the loss at default model, 
we perform recursive feature elimination. 

```{r, eval = FALSE}
# Set the recursive feature elimination parameters 
set.seed(1234)
rfe.cntrl = rfeControl(functions = rfFuncs,
                      method = "cv",
                      number = 5)
train.cntrl = trainControl(selectionFunction = "oneSE")

# Perform recursive feature elimination to select variables
rfe.results =
  rfe(percent_loss~.,
      data = train_with_defaults,
      rfeControl = rfe.cntrl,
      preProc = preProcessSteps,
      sizes =  seq(12,132,10), # commented out to reduce runtime 
      metric = "ROC",
      trControl = train.cntrl)

# Map factor levels back to their respective features 
selected_vars <- map(predictors(rfe.results), 
                      ~str_match(.x, names(train_with_defaults))) %>% 
  unlist() %>% 
  .[!is.na(.)] %>%
  unique()

# Create data frame with these selected variables 
train_selected_vars <- train_with_defaults %>%
  select(one_of(selected_vars), percent_loss)
```

#Model Fitting 

To tune hyperparameters, we use 5-fold cross-validation with the "one standard 
error" rule. 

```{r}
# Define cross-validation controls 
cvCtrl = trainControl(method = "cv", 
                       number = 5,
                       selectionFunction = "oneSE",
                       classProbs = TRUE)
                       # allowParallel = TRUE)
```

##Random Forest Model 
```{r}
# Define grid of tuning parameters
rfGrid = expand.grid(.mtry = c(2, 4, 6, 8, 10, 14))
# Fit random forest model
set.seed(1234)
rf.fit = train(percent_loss ~ .,
                   data = train_selected_vars,
                   preProc = preProcessSteps,
                   method = "rf",
                   tuneGrid = rfGrid, # commented out to reduce runtime
                   trControl = cvCtrl)
```

#Model Evaluation 

##Test Set Prediction  

We calculate the expected loss and default probability of each loan in the 
portfolio of 500 loans by using the model of expected loss and best model
of default probability. 

```{r}
# Make data frame with predicted loss and default probability for each loan
prediction_df = 
  bind_cols(
    # Predicted losses from loss at default model 
    data_frame(default_loss = predict(rf.fit, portfolio)) %>%
      # Apply sigmoid transformation to recover values between 0-1 
      mutate(default_loss = 1 / (1 + exp(1)^(-default_loss))),
    # Bind 1- and 5-year default probabilities
    default_probs) 

#Add GrossApproval for later computation of total loss
prediction_df$GrossApproval <-  portfolio$GrossApproval
```

##Simulate Distribution of Total Loss 
To estimate the value at risk, we generate simulations of the loan losses 
for the portfolio in batches. For each batch of 10000 portfolio simulations, 
we compute the value at risk and expected shortfall and store them. We then
take the average value at risk and calculate confidence interval for both 
metrics.

```{r}
# Initialize simulation parameters 
num_simulations = 10000
num_sim_batch = 100
num_loans = nrow(prediction_df)
var1yr_vec95 <- vector("numeric", num_sim_batch)
var1yr_vec99 <- vector("numeric", num_sim_batch)
var5yr_vec95 <- vector("numeric", num_sim_batch)
var5yr_vec99 <- vector("numeric", num_sim_batch)
etl1yr_vec95 <- vector("numeric", num_sim_batch)
etl1yr_vec99 <- vector("numeric", num_sim_batch)
etl5yr_vec95 <- vector("numeric", num_sim_batch)
etl5yr_vec99 <- vector("numeric", num_sim_batch)

for (j in c(1:num_sim_batch)){
  
  one_year_default_result <- vector("numeric", num_simulations)
  five_year_default_result <- vector("numeric", num_simulations)
  
  # run through simulations for the 500 loans
  for (simulation in c(1:num_simulations)) {
    one_year_loss = 0  
    five_year_loss = 0  
    
    # generate a uniform random variable and comparing that to our prediction 
    uniform_probs <- runif(num_loans)
    for (loan in c(1:num_loans)) {
      
       # if the uniform rv is smaller than one year default prob, loan has defaulted
      if (uniform_probs[loan] < prediction_df$one_year_prob[loan]) {
        one_year_loss = one_year_loss + prediction_df$default_loss[loan] * prediction_df$GrossApproval[loan]
      } 
      
       # if the uniform rv is smaller than five year default prob, loan has defaulted
      if (uniform_probs[loan] < prediction_df$five_year_prob[loan]) {
        five_year_loss = five_year_loss + prediction_df$default_loss[loan] * prediction_df$GrossApproval[loan]
      } 
    }
    
    # Update result vectors with total loss of current simulation 
    one_year_default_result[simulation] = one_year_loss
    five_year_default_result[simulation] = five_year_loss
  }
  
  # Convert result objects to data frames 
  portfolio_nominal <- sum(prediction_df$GrossApproval)
  one_year_default_result = data.frame(DollarAmount = one_year_default_result, 
                                       Percentage = one_year_default_result/portfolio_nominal)
  five_year_default_result = data.frame(DollarAmount = five_year_default_result, 
                                        Percentage = five_year_default_result/portfolio_nominal)
  
  #Compute Value at risk for every simulation batch
  var1yr_vec95[j] <- -VaR(-one_year_default_result$Percentage, p=0.95)[1]
  var1yr_vec99[j] <- -VaR(-one_year_default_result$Percentage, p=0.99)[1]
  var5yr_vec95[j] <- -VaR(-five_year_default_result$Percentage, p=0.95)[1]
  var5yr_vec99[j] <- -VaR(-five_year_default_result$Percentage, p=0.99)[1]
  
  #Compute expected shortfall for each batch
  etl1yr_vec95[j] <- -ETL(-one_year_default_result$Percentage, p=0.95)[1]
  etl1yr_vec99[j] <- -ETL(-one_year_default_result$Percentage, p=0.99)[1]
  etl5yr_vec95[j] <- -ETL(-five_year_default_result$Percentage, p=0.95)[1]
  etl5yr_vec99[j] <- -ETL(-five_year_default_result$Percentage, p=0.99)[1]
  cat("\nSim: ",j, " done")
}
```

##Plot Expected Loss Distributions
```{r}
# Plot the histogram for the 1-year losses 
ggplot(data = one_year_default_result, aes(one_year_default_result$Percentage)) +
  geom_histogram(position = "identity") +
  labs(x = "Total Loss at Default from 500 Loans", y = "Count", 
       title = "One-Year Expected Loss Distribution from 500 Loan Portfolio")
```



```{r}
# Plot the histogram for the 5-year losses 
ggplot(data = five_year_default_result, aes(five_year_default_result)) +
  geom_histogram(position = "identity") +
  labs(x = "Total Loss at Default from 500 Loans", y = "Count", 
       title = "Five-Year Expected Loss Distribution from 500 Loan Portfolio")
```

##Compute Value-at-Risk
We measure the risk in terms of the 1 year and 5 years ahead VaR at the 95% and 99% levels and include confidence intervals.

```{r}
#Data frame containing VaR metrics for each simulation
var1yr = data.frame(
  var1yr_vec95,
  var1yr_vec95*portfolio_nominal,
  var1yr_vec99,
  var1yr_vec99*portfolio_nominal)

var5yr = data.frame(
  var5yr_vec95,
  var5yr_vec95*portfolio_nominal,
  var5yr_vec99,
  var5yr_vec99*portfolio_nominal)

#list containing global VaR metrics
var1yr_95 <- list("Mean"= mean(var1yr_vec95),"Sd" = sd(var1yr_vec95))
var1yr_95$CI <- c(var1yr_95$Mean - 1.96 * var1yr_95$Sd / num_sim_batch,
                  var1yr_95$Mean + 1.96 * var1yr_95$Sd / num_sim_batch)

var1yr_99 <- list("Mean"= mean(var1yr_vec95),"Sd" = sd(var1yr_vec99))
var1yr_99$CI <- c(var1yr_99$Mean - 1.96 * var1yr_99$Sd / num_sim_batch,
                  var1yr_99$Mean + 1.96 * var1yr_99$Sd / num_sim_batch)

var5yr_95 <- list("Mean"= mean(var5yr_vec95),"Sd" = sd(var5yr_vec95))
var5yr_95$CI <- c(var5yr_95$Mean - 1.96 * var5yr_95$Sd / num_sim_batch,
                  var5yr_95$Mean + 1.96 * var5yr_95$Sd / num_sim_batch)

var5yr_99 <- list("Mean"= mean(var5yr_vec99),"Sd" = sd(var5yr_vec99))
var5yr_99$CI <- c(var5yr_99$Mean - 1.96 * var5yr_99$Sd / num_sim_batch,
                  var5yr_99$Mean + 1.96 * var5yr_99$Sd / num_sim_batch)

VaR_list <- list(var1yr, var5yr, var1yr_95, var1yr_99, var5yr_95, var5yr_99)
```

##Compute Average Value-at-Risk

We also measure the risk in terms of the 1 year and 5 years ahead verage VaR at the 95% and 99% levels and include confidence intervals

```{r}
ETL1yr = data.frame(
  etl1yr_vec95,
  etl1yr_vec95*portfolio_nominal,
  etl1yr_vec99,
  etl1yr_vec99*portfolio_nominal)

ETL5yr = data.frame(
  etl5yr_vec95,
  etl5yr_vec95*portfolio_nominal,
  etl5yr_vec99,
  etl5yr_vec99*portfolio_nominal)

etl1yr_95 <- list("Mean"= mean(etl1yr_vec95),"Sd" = sd(etl1yr_vec95))
etl1yr_95$CI <- c(etl1yr_95$Mean - 1.96 * etl1yr_95$Sd / num_sim_batch,
                  etl1yr_95$Mean + 1.96 * etl1yr_95$Sd / num_sim_batch)

etl1yr_99 <- list("Mean"= mean(etl1yr_vec95),"Sd" = sd(etl1yr_vec99))
etl1yr_99$CI <- c(etl1yr_99$Mean - 1.96 * etl1yr_99$Sd / num_sim_batch,
                  etl1yr_99$Mean + 1.96 * etl1yr_99$Sd / num_sim_batch)

etl5yr_95 <- list("Mean"= mean(etl5yr_vec95),"Sd" = sd(etl5yr_vec95))
etl5yr_95$CI <- c(etl5yr_95$Mean - 1.96 * etl5yr_95$Sd / num_sim_batch,
                  etl5yr_95$Mean + 1.96 * etl5yr_95$Sd / num_sim_batch)

etl5yr_99 <- list("Mean"= mean(etl5yr_vec99),"Sd" = sd(etl5yr_vec99))
etl5yr_99$CI <- c(etl5yr_99$Mean - 1.96 * etl5yr_99$Sd / num_sim_batch,
                  etl5yr_99$Mean + 1.96 * etl5yr_99$Sd / num_sim_batch)
ETL_list <- list(ETL1yr, ETL5yr, etl1yr_95, etl1yr_99, etl5yr_95, etl5yr_99)
```

#Interpretation and Risk Analysis

- To be discussed at Thursday meeting. 
- I think we should run this pipeline for portfolios from different time periods

#Loss Distributions by Tranche

- BEN