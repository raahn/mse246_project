---
title: "MS&E 246 Final Report"
author: "Samuel Hansen, Theo Vadpey, Alex Elkrief, Ben Etringer"
date: "2/23/2017"
output:
  pdf_document:
    toc: yes
  html_document:
    keep_md: yes
    number_sections: yes
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, 
                      message = FALSE, cache = TRUE, eval = TRUE)
```

```{r}
# Initialize libraries 
library(ggrepel)
library(knitr)
library(lubridate)
library(caret)
library(stringr)
library(scales)
library(plotROC)
library(pROC)
library(tidyverse)

# Initialize input files 
train_file_in <- "../data/train.rds"
test_file_in <- "../data/test.rds"
full_file_in <- "../data/merged.rds"
eda_file_in <- "../data/SBA_Loan_data_1.csv"

# Read in data for EDA
eda_data <- 
  read_csv(eda_file_in) %>%
  plyr::rename(replace = c("2DigitNAICS" = "NAICS")) %>%
  mutate(ApprovalDate = mdy(ApprovalDate))

# Read in data for EDA with external variables 
full_data <-
  read_rds(full_file_in)

# Read in train and test data 
train <- read_rds(train_file_in)
test <- read_rds(test_file_in)
```

#Exectutive Summary

In *MS&E 246: Financial Risk Analytics*, our team analyzed a data set of 
roughly 150,000 loans backed by the US Small Business Administration 
(SBA) between 1990 and 2014. In doing so, we aimed to implement and test models
of the risk and loss of loan default. This report summarizes our findings from 
exploratory data analysis, details our approaches to modeling loan 
default probability and loss, and presents our methods of estimating
the loss distributions of tranches backed by a portfolio of loans. 

#Exploratory Data Analysis

Prior to model building, we explored the data to detect patterns that may 
provide signal for models of loan default. Because we first aimed to build 
binary response models of default probability, we excluded "Exempt" loans from 
our exploratory analysis. Subsequently, we examined the relationship between 
default rates and the predictor variables, including `Business Type`, 
`Loan Amount`, `NAICS Code`, and `Loan Term`, among others. 

Further, we collected additional predictor variables such as monthly 
`GDP`, `Crime Rate`, and `Unemployment Rate` by State, as well as macroeconomic
predictors such as monthly measures of the `S&P 500`, `Consumer Price Index`, 
and 14 other volatility market indices (see "Data Cleaning" section for 
data collection details). We include insights from exploratory analysis of 
these measures as well. 

##Default Rate vs. Business Type 

First, we examined the relationship between default rate and `Business Type`
by loan approval year. As shown on the plot below, we observe an interaction
effect between these three features, such that default rates spiked for 
loans that were approved around the Great Recession (approximately 2006- 2009). 
Further, the different trajectories of the 3 curves implies the "Individual" 
`Business Type` suffered greater default rates than corporations and 
partnerships. Although corporations constitute a greater share of the data set,
as evidenced by the greater mass in the red circles, they exhibit medium 
default risk, as compared to the other business types. Taken together, 
this plot reveals business types were affected differently by the recession,
offering useful signal for subsequent modeling. 

```{r}
eda_data %>%
  group_by(BusinessType, year = year(ApprovalDate)) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = frac_defaulted, color = BusinessType)) +
  geom_point(mapping = aes(size = count)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(name = "Number of Loans",
                        labels = comma) +
  scale_color_discrete(name = "Business Type") + 
  labs(x = "Approval Year", y = "Default Rate",
       title = "Default Rate vs. Business Type by Approval Year")
```

##Default Rate by Loan Amount

Second, we examined whether we would observe a similar time-dependent 
interaction effect between default rate and `Loan Amount`. The plot below 
reveals that loans of all sizes approved around the Great Recession faced the
greatest default rates. However, loans of sizes \$500k-\$1m and \$1m-\$2m
appear to have experienced larger default rates over time compared to smaller
loans of size \$100k-\$300k and \$300k-\$500k. The spiking behavior of \$1m-\$2m
loans in 1999 and of loans greater than \$2m seem to be due to small sample 
sizes, as depicted by circle diameter. Overall, since loans of different sizes
have different default rate patterns over time, we would also expect 
the `Loan Amount` feature to offer predictive power. 

```{r}
loan_labels <- c("(-10,1e+05]" = "< 100k",
                 "(1e+05,3e+05]" = "100k - 300k",
                 "(3e+05,5e+05]" = "300k - 500k",
                 "(5e+05,1e+06]" = "500k - 1m",
                 "(1e+06,2e+06]" = "1m - 2m",
                 "(2e+06,4e+06]" = "> 2m")
eda_data %>%
  mutate(loan_bin = cut(GrossApproval, 
                        breaks = c(-10, 100000, 300000, 500000, 
                                   1000000, 2000000, 4000000)),
         loan_bin = plyr::revalue(loan_bin, loan_labels),
         year = year(ApprovalDate)) %>%
  group_by(loan_bin, year) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = frac_defaulted, color = loan_bin)) +
  geom_point(mapping = aes(size = count)) +
  geom_line() +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(name = "Number of Loans",
                        labels = comma) + 
  scale_color_discrete(name = "Loan Amount ($)") + 
  labs(x = "Approval Year", 
       y = "Default Rate", 
       title = "Default Rate by Loan Amount")
```

##Default Rate by NAICS Code

Third, we hypothesized different economic sectors would exhibit 
different default rates over time. In turn, we extracted the North American 
Industry Classification System (NAICS) code for each loan and truncated it
to the first two digits, which represents broad industry classes such as 
"Agriculture" and "Manufacturing." The following plot shows the default 
rate for loans of each truncated NAICS code approved in each year between 
1990-2014. We observe considerable variance in default rates between sectors;
for instance, code 72, corresponding to "Accommodation & Food Services", 
has one of the highest default rates even before the recession. However,
code 54, corresponding to "Professional, Scientific, and Technical Services,"
consistently has one the lowest default rates. These patterns are consistent
with intuition, and underscore the value of including the truncated NAICS code
as a predictive feature of defaulting. 

```{r}
eda_data %>%
  filter(NAICS != "00",
         NAICS != "99") %>%
  group_by(NAICS, year = year(ApprovalDate)) %>%
  summarise(default_rate = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = year, y = default_rate, color = NAICS)) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "NAICS Code") +
  scale_y_continuous(labels = scales::percent) +
  labs(x = "Approval Year", y = "Default Rate", title = "Default Rate by NAICS Code")
```

##Default Rate by Subprogram Type

Fourth, we compared the default rates between different loan subprogram types. 
The plot below shows the default rates of the different loan
subprograms versus their respective counts in the data. We observe 
that the PSF subprogram is the most common and has medium default risk. 
However, loans in the Premier Certified Lenders Program (PCLP) are less 
common, but have higher default risk. This suggests `Subprogram Type` offers 
useful signal for predicting default risk. Lastly, the loans belonging to the 
Delta and Refinance subprograms are highly uncommon and have low default risk. 
In order to reduce to the dimensionality of the feature space, we collapsed 
these two factor levels into "Other."

```{r}
eda_data %>%
  group_by(subpgmdesc) %>%
  summarise(count = n(),
            frac_defaulted = mean(LoanStatus == "CHGOFF")) %>%
  ggplot(mapping = aes(x = count, y = frac_defaulted, color = subpgmdesc)) +
  geom_point() +
  scale_y_continuous(labels = scales::percent) +
  scale_x_continuous(labels = scales::comma) +
  scale_color_discrete(name = "Subprogram Type") + 
  labs(x = "Count of Subprogram", y = "Default Rate",
       title = "Default Rate vs. Subprogram Type")
```

##State GDP vs. Default Rate

- Make plot here

#Modeling Default Probability 

Building upon our exploratory data analysis, we constructed two types of 
predictive models of loan default probability: binary response models and 
the Cox Proportional Hazards model. Here, we present our approach to fitting
both model types, including data cleaning, feature engineering, 
feature selection, hyper-parameter optimization, and evaluation. 

##Binary Response Models

First, we built binary response models of small-businesses defaulting on loans,
which estimate the probability that a given loan *ever* defaults. To do so,
we implemented a machine learning pipeline that: 

1. Performs feature engineering;
2. Splits the data into train and test sets;
3. Normalizes continuous features;
4. Selects features using recursive feature elimination;
5. Trains binary response predictive models. 

Lastly, we evaluated the performance of these models on resampled partitions 
of the training data, and on a held-out test set in terms of AUC, sensitivity, 
and calibration. 

###Feature Engineering

Building on insights derived from exploratory data analysis,
we engineered the following features from the raw data: 

- `NAICS_code`: truncated to the first two digits of the NAICS code;
- `subprogram`: condensed infrequent factor levels into "other" category;
- `approval_year`: extracted year from loan approval date-time object.
- `SameLendingState`: created flag for whether borrower received loan from in-state; 
- `MultiTimeBorrower`: created flag for whether loan recipient is a multi-time borrower;
- `ThirdPartyLender` created flag for whether borrower received third party aide. 

In effect, these features represent dimensionality reduction of factors 
with many levels. For instance, there are 1,239 unique NAICS six-digit NAICS
codes in the raw data, yet only 25 unique 2-digit codes. Although we lose 
fine-grained detail by truncating the NAICS code, we aimed to optimize our
models by reducing variance introduced by high dimensionality. After applying
such dimension reductions, we eliminated extraneous variables, such as the 
Borrower's Zip Code and the Project's State, which were used to engineer
features. 

In addition to constructing features from the raw data, we also incorporated 
data from external sources, including monthly State-based measures of 
crime rate, GDP, and unemployment rate. We also joined in time-varying risk 
factors, including monthly snapshots of the `S&P 500`, `Consumer Price Index`, 
and 14 other volatility market indices. 

- BEN: Fill in where the data came from and any other important info 

###Data Splitting 

We randomly partitioned the data into 70% training and 30% test sets. 
This approach does not implement a time-based split, but rather a random 
sampling of observations over the entire 1990-2014 window. We adopted this 
splitting approach because we were interested in capturing the signal 
of the Great Recession within our models. Further, we did not create a 
validation set because we performed feature selection and hyper-parameter
optimization using cross-validation on the training set. 

###Data Preprocessing

After engineering features and joining in external data sources, 
we applied several preprocessing steps to our main data frame.
First, we centered and scaled the continuous predictors to apply regularization 
techniques during the modeling phase. Doing so adjusted for variables being
on different scales; for example, `Gross Approval` varies in dollar amounts 
from \$30,000  to \$4,000,000, whereas `Term in Months` ranges from 1 to 389. 
Second, we applied a filter to remove features with near zero variance to 
eliminate predictors that do not offer meaningful signal. 

```{r}
# Define unnecessary videos 
vars_to_drop <- c("GrossChargeOffAmount", "ChargeOffDate", 
                             "ApprovalDate", "first_zip_digit")

# Remove unnecesary features for modeling 
train <- train %>% select(-one_of(vars_to_drop))
test <- test %>% select(-one_of(vars_to_drop))
```

```{r}
# Define pre-processing steps to apply to training data
preProcessSteps <- c("center", "scale", "nzv")

# Apply same pre-processing steps to the test set
preProcessObject <- preProcess(train, method = preProcessSteps)
train <- predict(preProcessObject, train)
test <- predict(preProcessObject, test)
```

###Feature Selection

To perform feature selection, we used recursive feature elimination
with 10-fold cross-validation. This method uses random forests to iteratively
remove variables with low variable importance, as measured by mean increase 
in out-of-bag area-under-the-curve (AUC). In other words, variables that 
do not contribute to significant improvements in AUC are eliminated. We
performed a grid search over the number of potential features to determine 
how many features to include. Note that factors were converted to separate dummy 
variables using a one-hot encoder. 

```{r, echo = FALSE}
rfe.results <- read_rds("../models/rfe.results.rds")
```

The following plot shows that recursive feature selection 
chose `r rfe.results[["results"]]$Variables[which.max(rfe.results[["results"]]$ROC)]` 
variables because AUC is maximized (see plot below). In effect, all variables
were kept because they offered predictive power regarding loan defaults. 

```{r}
ggplot(rfe.results) +
  labs(x = "Number of Variables",
       y = "AUC (Cross-Validated)",
       title = "Recursive Feature Elimination\nNumber of Variables vs. AUC")
```

The importances of the top 10 selected features are shown in the plot below.
We observe that State GDP, a monthly time-dependent risk factor, is the most 
important feature, meaning it led to the greatest average increase in AUC
across cross-validation iterations. State unemployment rate and crime rate 
are also highly important, suggesting local time-dependent risk factors 
are the most predictive of whether a loan defaults. 

The importance of NAICS code 72, corresponding to 
"Accommodation & Food Services", is consistent with our exploratory 
data analysis finding that the sector is especially risk prone. Borrower States
such as Michigan, California, and Florida also offer predictive power regarding 
defaulting. Lastly, the importances of the Collar Index (CLL) and Iron 
Butterfly Index (BFLY) imply market volatility measures also improve 
the discrimination of loan defaults. 

```{r}
data_frame(predictor = rownames(varImp(rfe.results)), 
           var_imp = varImp(rfe.results)$Overall) %>%
  slice(1:10) %>%
  ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "", 
       y = "Variable Importance", 
       title = "Recursive Feature Elimination Variable Importance")
```

###Model Fitting 

Using these selected features, we fit models predicting the binary outcome
of whether a small business defaults on a loan. We constructed linear and 
nonlinear models, including a logistic regression model with the elastic net 
penalty, a random forest classifier, and a gradient boosting machine 
classifier. To tune hyper-parameters, we used 10-fold cross-validation with the 
one standard error rule, which selects parameters that obtain the highest 
cross-validated AUC within one standard error of the maximum. For each model
type, we performed a grid search over the hyper-parameters to ensure optimal
selection.

####Logistic Regression with Elastic Net

First, AUC was used to select the optimal logistic regression model with an
elastic net penalty using the one standard-error rule. As shown in the plot 
below, the final values used for the model were `alpha` = 0.1 and `lambda` = 0,
indicated by the spike in the red curve at `alpha` = 0.1. This implies the 
optimal model used the ridge penalty more than the LASSO penalty with minimal 
regularization. 

```{r, echo = FALSE}
elastic.fit <- read_rds("../models/elastic.fit.rds")
elastic.fit$results %>%
  ggplot(mapping = aes(x = alpha, y = ROC, color = factor(lambda))) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "Lambda Penalty") +
  scale_x_continuous(breaks = seq(0, 1, 0.1)) +
  scale_y_continuous(breaks = seq(0.8, 0.83, 0.005)) +
  labs(x = "Mixing Percentage (Alpha)",
       y = "AUC (Cross-Validated)",
       title = "Logistic Regression Hyper-Parameter Optimization")
```

####Random Forest Classifier

Second, AUC was used to select the optimal random forest model, which selected 
`mtry` = 8 as the best parameter. This means 8 random predictors were chosen 
to build each tree of the random forest. The plot below shows steadily declining
AUC as the number of randomly chosen predictors increases, indicating that
the optimal model is sparsest. 

```{r, echo = FALSE}
rf.fit <- read_rds("../models/rf.fit.rds")
rf.fit$results %>%
  ggplot(mapping = aes(x = mtry, y = ROC)) +
  geom_point() +
  geom_line() +
  scale_color_discrete(name = "Lambda Penalty") +
  labs(x = "Number of Randomly Selected Predictors",
       y = "AUC (Cross-Validated)",
       title = "Random Forest Hyper-Parameter Optimization")
```

####Gradient Boosting Machine Classifier

Third, AUC was similarly used to select the optimal gradient boosting machine 
(GBM) model. The final values used for the model were `nrounds` = 100, 
`max_depth` = 6, `eta` = 0.03, `gamma` = 0, `colsample_bytree` = 0.4, 
`min_child_weight` = 1 and `subsample` = 0.5. This means that the tuning 
procedure utilized a learning rate of 0.03 and a minimum loss reduction of 0, 
resulting in the optimal model with 100 trees of maximum depth 6 that subsamples
50% of the observations and 40% of the features for each tree. This combination
of optimal hyper-parameters is shown by the spike of the red curve in the first 
subplot at the maximum tree depth of 6.  

```{r, echo = FALSE}
xgb.fit <- read_rds("../models/xgb.fit.rds")
subsamp_labs = c("0.5" = "Subsample = 50%",
                 "0.75" = "Subsample = 75%",
                 "1" = "Subsample = 100%")
xgb.fit$results %>%
  dmap_at("subsample", as.factor) %>%
  ggplot(mapping = aes(x = max_depth, y = ROC, color = factor(colsample_bytree))) +
  geom_point() +
  geom_line() +
  facet_grid(subsample~., labeller = labeller(subsample = subsamp_labs)) +
  scale_color_discrete(name = "Fraction of randomly\nselected features\nfor each tree") +
  labs(x = "Maximum Tree Depth", 
       y = "AUC (Cross-Validated)",
       title = "Gradient Boosting Hyper-Parameter Optimization")
```

Examining the variable importance of the final GBM model, we observe the most 
important feature for predicting defaults is the Collar Index (CLL), which is
"designed to provide investors with insights as to how one might protect an 
investment in S&P 500 stocks against steep market declines" ([CBOE](http://www.cboe.com/products/strategy-benchmark-indexes/collar-indexes/cboe-s-p-500-95-110-collar-index-cll)). Other important features include the national 
consumer price index (CPI), State GDP, crime, and unemployment rates, 
loan amount, and Chicago Board Options Exchange (CBOE) indices including the 
Butterfly Index (BFLY), the Iron Condor Index (CNDR), and the Volatility index 
(VIX). Such variables are "important" because they lead to the greatest 
improvements to cross-validated AUC across boosting iterations. 

```{r}
data_frame(predictor = rownames(varImp(xgb.fit)$importance), 
           var_imp = varImp(xgb.fit)$importance$Overall) %>%
  top_n(10) %>%
  ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "", 
       y = "Variable Importance", 
       title = "Gradient Boosting Machine Variable Importance")
```

###Model Evaluation 

After we optimized the hyper-parameters of our models, we evaluated the models  
using in-sample and out-of-sample metrics, including AUC, sensitivity,
ROC curves, and calibration. To do so, we used these "best" models to 
predict loan defaults in the training and test sets. 

####In-Sample Evaluation 

#####Training AUC and Sensitivity of Best Models 

The following plot compares averaged **training** area under the ROC curve 
and sensitivity across the model types with optimized parameters. We observe
that the gradient boosting machine classifier has the highest AUC and 
sensitivity, whereas the logistic regression model with the elastic net penalty
performs the worst. 

```{r}
# Evaluate performance of each model on training data
bind_rows(getTrainPerf(elastic.fit), 
          getTrainPerf(rf.fit),
          getTrainPerf(xgb.fit)) %>% 
  as_data_frame() %>%
  mutate(method = recode(method, 
                         "glmnet" = "Elastic Net", 
                         "rf" = "Random Forest",
                         "xgbTree" = "Gradient Boosting")) %>%
  ggplot(mapping = aes(x = TrainSens, y = TrainROC, label = method)) +
  geom_point() +
  geom_text_repel() +
  labs(x = "Train Sensitivity", y = "Training AUC", 
       title = "Training Sensitivity vs. AUC by Model Type")
```

#####Distribution of Resampled Training AUC, Sensitivity, and Specificity 

To examine the spread of **training** area under the ROC curve,
sensitivity, and specificity across model types, we leverage the resampled
data generated during the cross-validation of model fitting to plot 
their respective distributions. In the following plot, we observe that 
the GBM classifier has the highest median AUC, sensitivity, and specificity,
as well as the smallest spread. Although the random forest classifier has 
comparable sensitivity, it exhibits enormous variance compared to the other 
models, suggesting it is prone to overfitting. For this reason, the logistic
regression classifier (a linear model) outperforms the random forest classifier
(a non-linear model) in terms of AUC and specificity. 

```{r, fig.width=9, fig.height=6}
# Generate resamples from data 
resamps <- resamples(list(Elastic_Net = elastic.fit,
                          Random_Forest = rf.fit,
                          Gradient_Boosting = xgb.fit))

# Generate boxplots 
metric_labs = c("ROC" = "AUC", "Sens" = "Sensitivity", "Spec" = "Specificity")
resamps$values %>%
  gather(method, value, `Elastic_Net~ROC`:`Gradient_Boosting~Spec`) %>%
  separate(method, c("method", "metric"), sep = "~", remove = TRUE) %>%
  ggplot(mapping = aes(x = method, y = value)) +
  geom_boxplot() +
  facet_wrap(~metric, scales = "free_y", 
             labeller = labeller(metric = metric_labs)) +
  labs(x = "Model Type", y = "Metric Value", 
       title = "Spread of Training AUC, Sensitivity, and Specificity") +
  theme(axis.text.x=element_text(angle = 45, hjust = 1))

```

#####Training ROC Curves 

Lastly, we can examine the **training** ROC curves by model type. 
We observe that the random forest model has a near-perfect ROC curve,
which also implies it is overfitting to the training data. The GBM model again
performs worse than the random forest model on the training data, but likely 
because it is avoiding overfitting. The logistic regression model with the 
elastic net penalty performs the worst. 

```{r, fig.width=9, fig.height=6}
# Evaluate performance of trained models on training set 
trainResults <- data.frame(true_value = train$LoanStatus)
trainResults$randomForest <- predict(rf.fit, train, type = "prob")[,"default"]
trainResults$elasticNet <- predict(elastic.fit, train, type = "prob")[,"default"]
trainResults$gradientBoosting <- predict(xgb.fit, train, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = trainResults$randomForest,
                       response = trainResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = trainResults$elasticNet,
                       response = trainResults$true_value)),
             gradientBoosting = pROC::auc(roc(predictor = trainResults$gradientBoosting,
                       response = trainResults$true_value))) %>%
  gather(method, auc_value, randomForest:gradientBoosting) %>%
  mutate(auc_label = paste("AUC =", round(auc_value, 3)))

# Gather results in long format 
trainResults <- 
  trainResults %>%
  gather(method, predicted_prob, randomForest:gradientBoosting) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))

# Plot ROC curves by model type 
model_labels <- c("randomForest" = "Random Forest",
                  "elasticNet" = "Elastic Net",
                  "gradientBoosting" = "Gradient Boosting")
trainResults %>%
  ggplot(mapping = aes(d = true_value, m = predicted_prob)) +
  geom_roc(n.cuts = 5, labelsize = 2, labelround = 3) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  labs(x = "False Positive Fraction", y = "True Positive Fraction",
       title = "ROC Curves by Model Type") +
  facet_wrap(~method, labeller = labeller(method = model_labels)) +
  geom_text(data = aucs, aes(x = 0.75, y = 0.5, label = auc_label), 
                    colour = "black", inherit.aes = FALSE, parse = FALSE)
```

####Out-of-Sample Evaluation 

#####Test ROC Curves 

We evaluated our best models on a held-out test set representing 30% of the 
original data. The ROC curves below reveal that the GBM model performed 
the best on the test set, followed by the logistic regression model, and 
finally, the random forest classifier. The weak performance of the random forest 
classifier is likely due to overfitting on the training set. Nevertheless, 
all models achieve good performance over "random guessing" baselines. 

```{r}
# Evaluate performance of trained models on test set 
testResults <- data.frame(true_value = test$LoanStatus)
testResults$randomForest <- predict(rf.fit, test, type = "prob")[,"default"]
testResults$elasticNet <- predict(elastic.fit, test, type = "prob")[,"default"]
testResults$gradientBoosting <- predict(xgb.fit, test, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = testResults$randomForest,
                       response = testResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = testResults$elasticNet,
                       response = testResults$true_value)),
             gradientBoosting = pROC::auc(roc(predictor = testResults$gradientBoosting,
                       response = testResults$true_value))) %>%
  gather(method, auc_value, randomForest:gradientBoosting) %>%
  mutate(auc_label = paste("AUC =", round(auc_value, 3)))

# Gather results in long format 
testResults <- 
  testResults %>%
  gather(method, predicted_prob, randomForest:gradientBoosting) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))
```

```{r, fig.width=9, fig.height=6}
# Plot ROC curves by model type 
model_labels <- c("randomForest" = "Random Forest",
                  "elasticNet" = "Elastic Net",
                  "gradientBoosting" = "Gradient Boosting")
testResults %>%
  ggplot(mapping = aes(d = true_value, m = predicted_prob)) +
  geom_roc(n.cuts = 5, labelsize = 2, labelround = 3) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  labs(x = "False Positive Fraction", y = "True Positive Fraction",
       title = "ROC Curves by Model Type") +
  facet_wrap(~method, labeller = labeller(method = model_labels)) +
  geom_text(data = aucs, aes(x = 0.75, y = 0.5, label = auc_label), 
                    colour = "black", inherit.aes = FALSE, parse = FALSE)
```

#####Test Calibration Plots

Lastly, we evaluated the calibration of the our models' predicted probabilities
of loan default against the observed fraction of defaults in the data. 
A point on the dashed line means that the model's predicted probability
of default matched the empirical default rate. Points to the right of the line 
mean the model overestimated the default probability, whereas points to the 
left mean the model underestimated the default probability. 

The GBM model achieves the best calibration because its points follow 
the dashed line most closely. The logistic regression model with the elastic net
penalty achieves comparable performance; however, the random forest classifier
tends to overestimate default probabilities. Again, this weaker performance 
is likely due to overfitting. 

```{r, fig.width=10, fig.height=6}
# Make calibration plots, facetted by model type
pred_prob_midpoints <- data_frame(midpoint = c(0, 0, rep(seq(0.05, 0.875, 0.05), each = 3), 0.9, 0.95, 1))

testResults %>%
  mutate(prob_bin = cut_width(predicted_prob, width = 0.05)) %>%
  group_by(prob_bin, method) %>%
  dplyr::summarise(prob_default = mean(true_value, na.rm = TRUE),
            n = n()) %>%
  bind_cols(., pred_prob_midpoints) %>%
  ungroup() %>%
  ggplot(mapping = aes(x = midpoint, y = prob_default)) +
  geom_line() +
  geom_point(mapping = aes(size = n)) +
  # geom_text_repel(mapping = aes(color = "red")) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  scale_x_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_y_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_colour_discrete(guide = FALSE) +
  scale_size(name = "Number of\nPredictions",
             labels = scales::comma) +
  labs(x = "Predicted Default Probability (Bin Midpoint)",
       y = "Observed Default Fraction",
       title = "Calibration Plot: Predicted vs. Observed Default Probability") +
  facet_wrap(~method, labeller = labeller(method = model_labels))
```

The overfitting of the random forest classifier may be due to the fact that 
too many features were randomly selected to build trees at each
iteration. Our hyper-parameter optimization approach performed a grid search
over possible values of `mtry`, representing the number of features 
randomly chosen to build each tree in the forest. However, our grid may have
not been large enough, since the minimum value of `mtry` was chosen. 
However, computational resources limited our ability to refit models 
over a larger search space. 

Moreover, the gradient boosting machine classifier demonstrated the best
performance on the test set in terms of AUC and calibration.  

##Cox Proportional Hazards Model 

```{r load env, echo=F,results='hide',message=F}
load('../data/cox_data_environment_train.RData')
source('../scripts/cox_diagnostic_functions.R')
```
	
Survival analysis gives more detailed information about how the default risk of a loan varies over time. With binary classification, we estimated the probability that a given loan *ever* defaults. With a hazard model, we are able to estimate the probability that a loan defaults between any two points of time in its life.

###Model Choice

There exist many specialized Cox models that assume a particular form of the baseline hazard function. The Cox Proportional Hazards Model does not have this requirement. We can see this in the following description of the partial maximum likelihood procedure used to estimate the parameters of the Cox PH model:

The form of the cox model is:
    $$h(t) = h_0(t)exp(\beta^T X)$$

Suppose there are $r$ observed death times in the data (all distinct), and that $t_j$ is a death time in the set of possible death times: $R = \{t_1,t_2,...,t_r\}$. 

Then the conditional probability that an individual dies at time $t_j$ given $t_j$ is a time of death in the set $R$:
     $$\frac{P(\text{individual with feature vector $X^{(j)}$ dies at $t_j$})}{P(\text{one death at $t_j$})}$$     
     $$ = \frac{ P(T = t_j | X^{(j)}, T \geq t_j ) }{P(T = t_j | X^{(k_0)}, T \geq t_j) \cup P(T = t_j | X^{(k_1)}, T \geq t_j) \cup ... P(T = t_j | X^{(k_q)}, T \geq t_j)}$$

Where $k_0, ..., k_q$ correspond to the indices of observations with event times greater than or equal to $t_j$. Since the probabilities in the denominator are *assumed to be conditionally independent*, the denominator can be expressed as a sum of probabilities. Converting the above to continuous time, we get: 

$$ = \frac{\text{lim}_{\delta \rightarrow 0}\frac{P(T < t_j + \delta | X^{(j)}, T \geq t_j )}{\delta}}{ \sum_{i = k_0}^{k_q} \text{lim}_{\delta \rightarrow 0}\frac{P(T < t_j + \delta | X^{(i)}, T \geq t_j)}{\delta} }$$

$$ = \frac{h_j(t_j)}{\sum_{i = k_0}^{k_q} h_i(t_j)} = \frac{h_0(t_j) exp(\beta^T X^{(j)})}{\sum_{i = k_0}^{k_q} h_0(t_j) exp(\beta^T X^{(i)})} = \frac{exp(\beta^T X^{(j)})}{\sum_{i = k_0}^{k_q} exp(\beta^T X^{(i)})}$$

And we can see that the contribution of any observation to the likelihood function will not be dependent on $h_0(t)$. $\square$
	      
###Modifications to the Data

Roughly 95% of loans in the training data set had a term of 20 years. We decided that considering loans with the same term was more appropriate for this analysis (84,949 loans). 
Within the training data, about 86\% of loans were right censored (term did not expire in window, and did not default), about 7\% of loans were paid off (term expired in window), and about 7% of loans defaulted within the window (figure 1). 
		
```{r distribution, eval=T,echo=F,fig.height=3.5,fig.width=3.5,fig.cap="Loans in training data by status"}
descrip_status = table(status_m)
names(descrip_status) = c("right_censored","paid_off","defaulted")
par(cex.axis=0.85,cex.lab=0.85,cex=0.85)
barplot(descrip_status/length(status_m),
        ylab='percent of training data')
```	

Polynomial terms up to *degree five* were added for all numeric variables. Our intention was to include these features to capture non-linearities in these variables, and conduct feature selection during model fitting (through regularization).

Further, all numeric variables were centered to 0, and scaled by standard deviation.
	   
Missing values were set to 0 and an missing value indicator feature was added for each original variable.	   
      
Including expanded categorical variables, polynomials, and missing value dummies, the data had 201 features.
      
###Kaplan-Meier Survival Curves

A Kaplan-Meier curve is a non-parametric estimate of the survival function, $S(t) = P(T>t)$, defined as: 

$$\hat{S(t)} = \prod_{t_i\leq t}\big[1 - \frac{d_i}{n_i}\big]$$

Where $\{t_1,...,t_r\}$ are the death times of observations in the data, $\{d_1,...,d_r\}$ are the number of deaths that occur at those times, and $\{n_1,...,n_r\}$ are the number of observations remaining in the at-risk population just before those times.

For expository purposes the following plots show the estimated survival function conditioned on select categorical variables such as a particular year, state, or status, as well as the general survival curve for our loan population. Note that the survival curve was significantly steeper for loans conditioned on these variables (a higher probability of default at all times).
	     
\begin{center}
\includegraphics[width=630pt]{../studies/surv_curvs.pdf}	
\end{center}	
		
###Penalized Cox Proportional Hazards Model

For the purpose of feature selection, we fit a series of penalized Cox models to the training data.

We used an elastic net penalty-- a penalty term that is a linear combination of the $l_1$ and $l_2$ penalties.
   $$\lambda [ (1-\alpha)||\beta||_2 + \alpha||\beta||_1]$$
   
We fit models varying $\alpha$ and $\lambda$ in the penalty-- we selected the model with the largest evaluated value of the likelihood function.

\begin{center}      
\includegraphics[width=350pt,height=620pt]{../studies/heatmap.pdf}
\end{center}
	
The best model, in terms had a value of $\lambda$ very close to 0, and $\alpha$ very close to 0 (the ridge penalty). Ninety-seven variables of the original 201 had non-zero coefficients.
    
###One Year and Five Year Predictions of Default (out of sample)

The below figures show the out of sample performance of the one and five year 
probabilities estimated by the Cox model:   

\begin{center}   
\includegraphics[width=315pt,height=300pt]{../studies/p_1_roc_curve.pdf}
\includegraphics[width=315pt,height=300pt]{../studies/p_5_roc_curve.pdf}
\end{center}

#Modeling Loss at Default 

Using our optimal Cox proportional hazards model, we computed the 
value-at-risk (VaR) and average value-at-risk (AVaR) for a portfolio of 500 
randomly chosen loans. Here, we detail our procedure for selecting a loan
portfolio, constructing a model for loss at default, simulating the total
loss of the portfolio, and computing VaR and AVaR. 

##Portfolio Selection

To build a model for loss at default, we considered a portfolio of 500 loans 
selected from the withheld test data set. These loans met the following criteria:
    
1. Loans that had not defaulted as of 02-01-2010.
2. Loans that were approved before 02-01-2010.
3. Loans less than 15 years old.

These conditions were to ensure that the 500 loans in question were active as 
of the portfolio construction date, which we determined to be 02-01-2010. The 
15 year age limit was so that estimation of 5 year ahead default probabilities 
would be valid.  

##Data Cleaning

Before fitting the loss at default model, we cleaned the training set 
by filtering it to only include defaulted loans and by removing unnecessary
features such as `LoanStatus`. 

```{r eval = FALSE}
# Prepare train data frame to only include defaulted loans 
cols_to_drop_train  = c("LoanStatus", "ChargeOffDate", "ApprovalDate", 
                  "first_zip_digit", "GrossChargeOffAmount", "GrossApproval")

train_with_defaults = 
  train %>%
  # Filter to only include defaulted loans
  filter(LoanStatus == "default") %>%
  # Make percentage loss column 
  mutate(percent_loss = GrossChargeOffAmount/GrossApproval) %>%
  # Filter out percent loss above 1
  filter(percent_loss <= 1) %>%
  # Remove unecessary variables 
  select(-one_of(cols_to_drop_train))

cols_to_drop_df  = c("LoanStatus", "ChargeOffDate", "ApprovalDate", 
                  "first_zip_digit", "GrossChargeOffAmount")
# Prepare portfolio data frame to match training variables 
portfolio = 
  portfolio %>%
  # Make percentage loss column 
  mutate(percent_loss = GrossChargeOffAmount/GrossApproval) %>%
  # Remove unecessary variables 
  select(-one_of(cols_to_drop_df)) 

# Convert probabilities object to data frame 
default_probs = 
  data_frame(one_year_prob = p_1 %>% unlist(),
             five_year_prob = p_5 %>% unlist()) 
```

```{r eval = FALSE}
# Define pre-processing steps
preProcessSteps = c("nzv")
```

##Feature Selection

To select the features used in the loss at default model, we performed 
recursive feature elimination. We used 5-fold cross validation and the 
"one standard error rule" to choose the number of features that minimized
mean squared error within one standard error of the minimum. 

```{r eval = FALSE}
# Set the recursive feature elimination parameters 
set.seed(1234)
rfe.cntrl = rfeControl(functions = rfFuncs,
                      method = "cv",
                      number = 5)
train.cntrl = trainControl(selectionFunction = "oneSE")

# Perform recursive feature elimination to select variables
rfe.results =
  rfe(percent_loss~.,
      data = train_with_defaults,
      rfeControl = rfe.cntrl,
      preProc = preProcessSteps,
      sizes =  seq(12,132,10), # commented out to reduce runtime 
      metric = "ROC",
      trControl = train.cntrl)

# Map factor levels back to their respective features 
selected_vars <- map(predictors(rfe.results), 
                      ~str_match(.x, names(train_with_defaults))) %>% 
  unlist() %>% 
  .[!is.na(.)] %>%
  unique()

# Create data frame with these selected variables 
train_selected_vars <- train_with_defaults %>%
  select(one_of(selected_vars), percent_loss)
```

##Model Fitting 

Using the features selected by recursive feature elimination,
we built a random forest model of loss at default. We used 5-fold 
cross-validation and the one standard error rule to find the optimal
number of features to be considered for splitting during construction of each
tree. 

```{r eval = FALSE}
# Define cross-validation controls 
cvCtrl = trainControl(method = "cv", 
                       number = 5,
                       selectionFunction = "oneSE",
                       classProbs = TRUE)
                       # allowParallel = TRUE)
```

```{r eval = FALSE}
# Define grid of tuning parameters
rfGrid = expand.grid(.mtry = c(2, 4, 6, 8, 10, 14))
# Fit random forest model
set.seed(1234)
rf.fit = train(percent_loss ~ .,
                   data = train_selected_vars,
                   preProc = preProcessSteps,
                   method = "rf",
                   tuneGrid = rfGrid, # commented out to reduce runtime
                   trControl = cvCtrl)
```

##Model Evaluation 

###Test Set Prediction  

We calculate the expected loss and default probability of each loan in the 
portfolio of 500 loans by using the model of expected loss and best model
of default probability. 

```{r eval = FALSE}
# Make data frame with predicted loss and default probability for each loan
prediction_df = 
  bind_cols(
    # Predicted losses from loss at default model 
    data_frame(default_loss = predict(rf.fit, portfolio)) %>%
      # Apply sigmoid transformation to recover values between 0-1 
      mutate(default_loss = 1 / (1 + exp(1)^(-default_loss))),
    # Bind 1- and 5-year default probabilities
    default_probs) 

#Add GrossApproval for later computation of total loss
prediction_df$GrossApproval <-  portfolio$GrossApproval
```

```{r echo=FALSE}
prediction_df = read_rds('../data/alex_prediction_df.rds')
```

###Simulate Distribution of Total Loss 

To estimate the value at risk, we generate simulations of the loan losses 
for the portfolio in batches. For each batch of 10'000 portfolio simulations, 
we compute the value at risk and expected shortfall and store them. We then
take the average value at risk and calculate confidence interval for both 
metrics.

```{r eval = FALSE}
# Initialize simulation parameters 
num_simulations = 10000
num_sim_batch = 100
num_loans = nrow(prediction_df)
var1yr_vec95 <- vector("numeric", num_sim_batch)
var1yr_vec99 <- vector("numeric", num_sim_batch)
var5yr_vec95 <- vector("numeric", num_sim_batch)
var5yr_vec99 <- vector("numeric", num_sim_batch)
etl1yr_vec95 <- vector("numeric", num_sim_batch)
etl1yr_vec99 <- vector("numeric", num_sim_batch)
etl5yr_vec95 <- vector("numeric", num_sim_batch)
etl5yr_vec99 <- vector("numeric", num_sim_batch)

for (j in c(1:num_sim_batch)){
  
  one_year_default_result <- vector("numeric", num_simulations)
  five_year_default_result <- vector("numeric", num_simulations)
  
  # run through simulations for the 500 loans
  for (simulation in c(1:num_simulations)) {
    one_year_loss = 0  
    five_year_loss = 0  
    
    # generate a uniform random variable and comparing that to our prediction 
    uniform_probs <- runif(num_loans)
    for (loan in c(1:num_loans)) {
      
       # if the uniform rv is smaller than one year default prob, loan has defaulted
      if (uniform_probs[loan] < prediction_df$one_year_prob[loan]) {
        one_year_loss = one_year_loss + prediction_df$default_loss[loan] * prediction_df$GrossApproval[loan]
      } 
      
       # if the uniform rv is smaller than five year default prob, loan has defaulted
      if (uniform_probs[loan] < prediction_df$five_year_prob[loan]) {
        five_year_loss = five_year_loss + prediction_df$default_loss[loan] * prediction_df$GrossApproval[loan]
      } 
    }
    
    # Update result vectors with total loss of current simulation 
    one_year_default_result[simulation] = one_year_loss
    five_year_default_result[simulation] = five_year_loss
  }
  
  # Convert result objects to data frames 
  portfolio_nominal <- sum(prediction_df$GrossApproval)
  one_year_default_result = data.frame(DollarAmount = one_year_default_result, 
                                       Percentage = one_year_default_result/portfolio_nominal)
  five_year_default_result = data.frame(DollarAmount = five_year_default_result, 
                                        Percentage = five_year_default_result/portfolio_nominal)
  
  #Compute Value at risk for every simulation batch
  var1yr_vec95[j] <- -VaR(-one_year_default_result$Percentage, p=0.95)[1]
  var1yr_vec99[j] <- -VaR(-one_year_default_result$Percentage, p=0.99)[1]
  var5yr_vec95[j] <- -VaR(-five_year_default_result$Percentage, p=0.95)[1]
  var5yr_vec99[j] <- -VaR(-five_year_default_result$Percentage, p=0.99)[1]
  
  #Compute expected shortfall for each batch
  etl1yr_vec95[j] <- -ETL(-one_year_default_result$Percentage, p=0.95)[1]
  etl1yr_vec99[j] <- -ETL(-one_year_default_result$Percentage, p=0.99)[1]
  etl5yr_vec95[j] <- -ETL(-five_year_default_result$Percentage, p=0.95)[1]
  etl5yr_vec99[j] <- -ETL(-five_year_default_result$Percentage, p=0.99)[1]
}
```

##Plot Expected Loss Distributions

```{r echo=FALSE}
one_year_default_result = read_rds('../data/alex_one_year_default_result.rds')
five_year_default_result = read_rds('../data/alex_five_year_default_result.rds')
```

The following plots show the Total loss distribution in percentage of the
total porftolio nominal for 100'000 portfolio simulations. Further, we get
an average loss of `r mean(one_year_default_result$Percentage*100)`% for the 
one year ahead period and `r mean(five_year_default_result$Percentage*100)`% for
five years.

```{r}
# Plot the histogram for the 1-year losses 
ggplot(data = one_year_default_result, aes(one_year_default_result$Percentage*100)) +
  geom_histogram(position = "identity") +
  labs(x = "Total Loss at Default from 500 Loans", y = "Count", 
       title = "One-Year Expected Loss Distribution from 500 Loan Portfolio")
```

```{r}
# Plot the histogram for the 5-year losses 
ggplot(data = five_year_default_result, aes(five_year_default_result$Percentage*100)) +
  geom_histogram(position = "identity") +
  labs(x = "Total Loss at Default from 500 Loans", y = "Count", 
       title = "Five-Year Expected Loss Distribution from 500 Loan Portfolio")
```

##Compute Value-at-Risk

Following the simulations, the table below shows the VaR results and the 95 and
99% level with a 95% confidence interval for one and five years respectively.

```{r echo=FALSE}
#loading stored VaR estimates
VaR_list = read_rds('../data/alex_var_list.rds')
ETL_list = read_rds('../data/alex_etl_list.rds')
```

```{r eval = FALSE}
#Data frame containing VaR metrics for each simulation
var1yr = data.frame(
  var1yr_vec95,
  var1yr_vec95*portfolio_nominal,
  var1yr_vec99,
  var1yr_vec99*portfolio_nominal)

var5yr = data.frame(
  var5yr_vec95,
  var5yr_vec95*portfolio_nominal,
  var5yr_vec99,
  var5yr_vec99*portfolio_nominal)

#list containing global VaR metrics
var1yr_95 <- list("Mean"= mean(var1yr_vec95),"Sd" = sd(var1yr_vec95))
var1yr_95$CI <- c(var1yr_95$Mean - 1.96 * var1yr_95$Sd / num_sim_batch,
                  var1yr_95$Mean + 1.96 * var1yr_95$Sd / num_sim_batch)

var1yr_99 <- list("Mean"= mean(var1yr_vec99),"Sd" = sd(var1yr_vec99))
var1yr_99$CI <- c(var1yr_99$Mean - 1.96 * var1yr_99$Sd / num_sim_batch,
                  var1yr_99$Mean + 1.96 * var1yr_99$Sd / num_sim_batch)

var5yr_95 <- list("Mean"= mean(var5yr_vec95),"Sd" = sd(var5yr_vec95))
var5yr_95$CI <- c(var5yr_95$Mean - 1.96 * var5yr_95$Sd / num_sim_batch,
                  var5yr_95$Mean + 1.96 * var5yr_95$Sd / num_sim_batch)

var5yr_99 <- list("Mean"= mean(var5yr_vec99),"Sd" = sd(var5yr_vec99))
var5yr_99$CI <- c(var5yr_99$Mean - 1.96 * var5yr_99$Sd / num_sim_batch,
                  var5yr_99$Mean + 1.96 * var5yr_99$Sd / num_sim_batch)

VaR_list <- list(var1yr, var5yr, var1yr_95, var1yr_99, var5yr_95, var5yr_99)
```

![Value at Risk results](../studies/alex_var_table.png)


##Compute Average Value-at-Risk

Similarly to the Value at Risk we compute the same metrics for the Average Value at Risk, 
also named Expected Tail loss. This metric represents the expected loss on the portfolio 
in the worst 1% and 5% of scenarios respectively. Again, we do this for 1 year and 5 year
simulations.

```{r eval = FALSE}
ETL1yr = data.frame(
  etl1yr_vec95,
  etl1yr_vec95*portfolio_nominal,
  etl1yr_vec99,
  etl1yr_vec99*portfolio_nominal)

ETL5yr = data.frame(
  etl5yr_vec95,
  etl5yr_vec95*portfolio_nominal,
  etl5yr_vec99,
  etl5yr_vec99*portfolio_nominal)

etl1yr_95 <- list("Mean"= mean(etl1yr_vec95),"Sd" = sd(etl1yr_vec95))
etl1yr_95$CI <- c(etl1yr_95$Mean - 1.96 * etl1yr_95$Sd / num_sim_batch,
                  etl1yr_95$Mean + 1.96 * etl1yr_95$Sd / num_sim_batch)

etl1yr_99 <- list("Mean"= mean(etl1yr_vec95),"Sd" = sd(etl1yr_vec99))
etl1yr_99$CI <- c(etl1yr_99$Mean - 1.96 * etl1yr_99$Sd / num_sim_batch,
                  etl1yr_99$Mean + 1.96 * etl1yr_99$Sd / num_sim_batch)

etl5yr_95 <- list("Mean"= mean(etl5yr_vec95),"Sd" = sd(etl5yr_vec95))
etl5yr_95$CI <- c(etl5yr_95$Mean - 1.96 * etl5yr_95$Sd / num_sim_batch,
                  etl5yr_95$Mean + 1.96 * etl5yr_95$Sd / num_sim_batch)

etl5yr_99 <- list("Mean"= mean(etl5yr_vec99),"Sd" = sd(etl5yr_vec99))
etl5yr_99$CI <- c(etl5yr_99$Mean - 1.96 * etl5yr_99$Sd / num_sim_batch,
                  etl5yr_99$Mean + 1.96 * etl5yr_99$Sd / num_sim_batch)
ETL_list <- list(ETL1yr, ETL5yr, etl1yr_95, etl1yr_99, etl5yr_95, etl5yr_99)
```

![Expected Tail loss results](../studies/alex_etl_table_png.png)

##Interpretation and Risk Analysis

- To be discussed at Thursday meeting. 
- I think we should run this pipeline for portfolios from different time periods

```{r echo=FALSE}
ggplot(data = one_year_default_result, aes(one_year_default_result$Percentage*100)) +
  geom_histogram(position = "identity") +
  geom_vline(xintercept = VaR_list[3][[1]]$Mean*100, linetype = "longdash", show.legend = T, color="darkgreen") + 
  geom_vline(xintercept = VaR_list[4][[1]]$Mean*100, linetype = "longdash", show.legend = T, color="darkgreen") + 
  geom_vline( xintercept = ETL_list[3][[1]]$Mean*100, linetype = "longdash", show.legend = T, color="red") + 
  geom_vline(xintercept = ETL_list[4][[1]]$Mean*100, linetype = "longdash", show.legend = T, color="red") + 
  labs(x = "Total Loss at Default from 500 Loans", y = "Count", 
       title = "One-Year Expected Loss Distribution from 500 Loan Portfolio")


```



#Loss Distributions by Tranche

In this section, we will estimate the distribution for the one and five year losses of an investor who has purchased a [5%, 15%] tranche backed by the 500 loan portfolio.  We will also investigate the loss distribution of the [15%, 100%] senior tranche for the given portfolio.  In addition, we will estimate the yearly distribution of observed losses for the one and five year tranches for the set of all 500 loan portfolios in a given year from 1990-2013.

##Portfolio and assumptions

In the first task, we use the 500 loan portfolio descibed in the previous section.  For the second task, we assume that all active loans whose term length does not expire within the 1- and/or 5-year window are eligible for the tranche.  We select from the dataframe of total loans, a subset of active loans that meet this requirement.  We neglect pre-payment of loans and ignore accrued interest when determining yearly cashflow.

```{r eval=FALSE}
# Clear our workspace
graphics.off()
rm(list = ls())

# Read in our data files
df = readRDS('../data/tranche_prob_df.rds')
df2 = readRDS('../data/merged.rds')

# Create new data frame [Approval Year, Approval Amount, Default Year (NA if no def), Termination Year]
mat = rep(0,0); # Initialize
mat = cbind(mat, as.numeric(levels(df[,3]))[df[,3]]); # Append approval year
mat = cbind(mat, df[,4]); # Append approval amt
mat = cbind(mat, as.numeric(format(df2$ChargeOffDate, '%Y'))); # Append Default Year
mat = cbind(mat, as.numeric(levels(df[,3]))[df[,3]] + ceiling(df2$TermInMonths/12)) # Append term year

# Determine active loans for given year 
year = 1990
# Initialize matrices for active loans
active1 = rep(0,0);
active5 = rep(0,0);
# Loop over entire dataframe of loans
for (i in 1:54794){
  # If approval year <= test year AND it did not default prior to test year
  if ((mat[i,1] <= year) && (is.na(mat[i,3]) || mat[i,3]>= year)){
    # If the loan has not expired within a year
    if (mat[i,4] >= year+1){
      # Append loan to active loans for 1-year
      active1 = rbind(active1, mat[i,])
    }
    # If the loan has not expired within 5 years
    if (mat[i,4] >= year+5){
      # Append loan to active loans for 5-year
      active5 = rbind(active5, mat[i,])
    }
  }
}
```

###Selection of loans for portfolio

Once the dataframe of eligible loans for the portfolio has been created, we select 500 loans uniformly random from the list.  We store the 500 loans in a matrix in R.  

```{r eval=FALSE}
# Pick 500 loans uniformly random from active loan list
sim1 = sample(1:dim(active1)[1], 500)
sim1 = active1[sim1,]
sim5 = sample(1:dim(active5)[1], 500)
sim5 = active5[sim5,]
```

###Determine value of the portfolio of loans 

Once we have our loans selected for our portfolio, we determine the value of the portfolio, which is driven by the annual expected cashflow assuming zero defaults.  It may seem intuitive to simply add the value of each loan for the portfolio to determine the value of the tranche.  However, this method would not account for different term lengths.  For example, a loan for \$100,000 over 1-year would be more be more valuable in the 1-year tranche than a 5-year loan for \$200,000.  We account for this problem by normalizing the value of each loan by the term length.  That is,

$$ V_i = \frac{A_i}{\Delta_t}, $$

where $V_i$ is the cashflow value of the $i-$th loan in the portfolio, and $\Delta_t$ is the portfolio duration (1- or 5-year in our case).  Note, this will ignore minor discrepancies between accrued interest.  In addition, we assume that loans either default or are paid in full at the loan termination date.  Note, this assumption ignores the possibility of a borrower paying the loan off before the loan due date.

```{r eval=FALSE}
# Determine total value of portfolio
value1 = sum(sim1[,2]/(sim1[,4]-sim1[,1]))
value5 = sum(sim5[,2]/(sim5[,4]-sim5[,1]))
```

###Determine the loss from the portfolio of loans

In an identical manner to determining the value of the portfolio, we will determine the loss observed by the portfolio. Note, we must account for borrowers who made payments before defaulting.  Thus,

$$ l_i = \frac{t_{\text{def},i}-t_0}{\Delta_t} V_i, $$

where $L_i$ is the loss value given by the $i-$th defaulting loan in the portfolio, $t_{\text{def},i}$ is the year of default for the $i-$th loan, $t_0$ is the year of the initiation of the portfolio.  Then the percent loss is given by,

$$ L = \frac{\sum_i l_i}{\sum_i A_i} $$ 

```{r eval=FALSE}
# Determine default loans 
def1 = sim1[!is.na(sim1[,3]),]
def5 = sim5[!is.na(sim5[,3]),]

# Determine total loss
loss1 = sum(def1[,2]/(def1[,4]-def1[,1]))
loss5 = 0;
if (is.null(dim(def5))){
  loss5 = def5[2]/(def5[4]-def[1])
} else {
  if ((dim(def5)[1]) > 1){
    for (i in 1:(dim(def5)[1])){
      loss5 = loss5 + def5[i,2]/(def5[i,4]-def5[i,1])
    }
  }
}
# Determine absolute ratio of loss
loss_percent1 = c(loss_percent1, loss1/value1);
loss_percent5 = c(loss_percent5, loss5/value5);
```

###Generate loss distribution and plotting
In this subsection, we will generate distribution plots and interpret the results.

####Predicted distribution of the 500-loan portfolio by tranches
We use the vector of percent losses, where each element is the loss from a single iteration of the simulation.  We transform the absolute loss percentage into a loss percentage for the tranche.  This transform is given by

\begin{equation}
  f(x)=\begin{cases}
    0, & \text{if $L<a$}.\\
    1, & \text{if $L>b$}. \\
    \frac{1}{b-a}(L-a), &\text{otherwise}.
  \end{cases}
\end{equation}

where $[a, b]$ are the bounds of the tranche, and $L$ is the absolute percent loss. 

####Observed yearly distributions of 500-loan portfolios by tranches
We run this simulation of selected 500 loans uniformly random from the list of active loans 1000 times, and compute the appropriate losses for each tranche.  We then plot the approximated distribution using the Kernel Density Estimator (KDE) with bounded [0,1] support.

```{r eval=FALSE}
# Initialize Vector of Tranche Losses
tranche1_5_15 = rep(0, length(loss_percent1))
tranche1_15_100 = rep(0, length(loss_percent1))
tranche5_5_15 = rep(0, length(loss_percent1))
tranche5_15_100 = rep(0, length(loss_percent5))

# Tranche [.05, .15] loss for 1 year
for (i in 1:length(loss_percent1)){
  if (loss_percent1[i]<.05){
    tranche1_5_15[i] = 0;
  } else {
    if (loss_percent1[i] > 0.15){
      tranche1_5_15[i] = 1;
    } else{
      tranche1_5_15[i] = 10*(loss_percent1[i] - .05)
    }
  }
}

# Tranche [.05, .15] loss for 5 year
for (i in 1:length(loss_percent5)){
  if (loss_percent5[i]<.05){
    tranche5_5_15[i] = 0;
  } else {
    if (loss_percent5[i] > 0.15){
      tranche5_5_15[i] = 1;
    } else{
      tranche5_5_15[i] = 10*(loss_percent5[i] - .05)
    }
  }
}

# Tranche [.15, 1] loss for 1 year
for (i in 1:length(loss_percent1)){
  if (loss_percent1[i]<.15){
    tranche1_15_100[i] = 0;
  } else {
    tranche1_15_100[i] = 100/85*(loss_percent1[i] - .15)
  }
}

# Tranche [.15, 1] loss for 1 year
for (i in 1:length(loss_percent5)){
  if (loss_percent5[i]<.15){
    tranche5_15_100[i] = 0;
  } else {
    tranche5_15_100[i] = 100/85*(loss_percent5[i] - .15)
  }
}

# Plot results and save figures
y_max = max(c(max(density(tranche1_5_15, bw = .0075, from = 0, to = 1)$y), max(density(tranche5_15_100, bw = .0075, from = 0, to = 1)$y)))
#png(filename = paste("./DefaultDist/full", year, "tranche.png", sep = "_"))
plot(density(tranche1_5_15, bw = .0075, from = 0, to = 1), 
     main = paste("Loss Distribution for 1- and 5-year in", year, "for tranches [5,15], [15,100]", sep = " "), 
     xlab = "Loss", ylab = "Density", 
     xlim = c(0,1), ylim = c(0, min(c( 15, y_max))), col = 'blue');
lines(density(tranche5_5_15, bw = .0075, from = 0, to = 1), col = 'red')
lines(density(tranche1_15_100, bw = .0075, from = 0, to = 1), col = 'green')
lines(density(tranche5_15_100, bw = .0075, from = 0, to = 1), col = 'orange')
legend('top', c('1-yr Tranche [5,15]', '5-yr Tranche [5,15]', '1-yr Tranche [15,100]', '5-yr Tranche [15,100]'), col = c('blue', 'red', 'green', 'orange'), lwd = 2)
#dev.off()
```
![Tranche loss distribution for 1998](../scripts/Tranche/DefaultDist/full_1998_tranche.png)
![Tranche loss distribution for 2003](../scripts/Tranche/DefaultDist/full_2003_tranche.png)
![Tranche loss distribution for 2008](../scripts/Tranche/DefaultDist/full_2008_tranche.png)


##Interpretations and Comparison of Distributions

See below table
![Distribution statistics from tranche losses by year](../scripts/Tranche/DefaultDist/analysis_table.png)

We can see from the approximated density plots of the observed losses seen by randomly generated portfolios, and the above table, that up until 2006 the [5%, 15%] tranche was equally risky as the [15%, 100%] tranche.  All of the randomly generated portfolios generated a 0% loss in the [5%, 15%] tranche through 2005.  The senior tranche obtained zero loss throughout the 1990-2013 timeframe.  However, from 2007-2010, the [5%, 15%] 5-year tranche receives significant loss (up to 100% in 2009-10). From a risk management point of view, an individual who is completely risk-averse would be willing to invest in the [5%, 15%] tranche prior to year 2007.  However, the risk-averse investor would have to switch to the senior tranche after 2007 in order to maintain the desired risk portfolio.  

We see that during the financial crisis, the observed losses exceed the predicted losses from the model.  In the previous section, we determined that the 5Y VaR at the 95% confidence level was 5.75%, and 6.49% at the 99% level.  However, the portfolio actually observed a 6.10% loss in 5-years.  While this is within the tolerance for the 99% confidence interval, we can suspect that the financial crisis led to the increased observed losses compared to the predicted losses.
