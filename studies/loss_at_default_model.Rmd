---
title: "Loss at Default Model"
author: "Samuel Hansen"
date: "2/23/2017"
output: 
  html_document:
    toc: true
    keep_md: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, 
                      message = FALSE, cache = TRUE, eval = FALSE)
```

```{r}
# Initialize libraries 
library(knitr)
library(caret)
library(stringr)
library(tidyverse)

# Initialize input files 
train_file_in = "../data/train.rds"
portfolio_file_in = "../data/portfolio.rds"
prob_file_in <- "../data/cox_portfolio_probabilities.RData"

# Read in data 
train = read_rds(train_file_in)
portfolio = read_rds(portfolio_file_in)
load(prob_file_in)
```

#Data Preparation

##Cleaning

Before fitting the loss at default model, we clean the training set 
by filtering it to only include defaulted loans and by removing unnecessary
features such as `LoanStatus`. 

```{r}
cols_to_drop  = c("LoanStatus", "ChargeOffDate", "ApprovalDate", 
                  "first_zip_digit", "GrossChargeOffAmount", "GrossApproval")
# Prepare train data frame to only include defaulted loans 
train_with_defaults = 
  train %>%
  # Filter to only include defaulted loans
  filter(LoanStatus == "default") %>%
  # Make percentage loss column 
  mutate(percent_loss = GrossChargeOffAmount/GrossApproval) %>%
  # Remove unecessary variables 
  select(-one_of(cols_to_drop))
  
# Prepare portfolio data frame to match training variables 
portfolio = 
  portfolio %>%
  # Make percentage loss column 
  mutate(percent_loss = GrossChargeOffAmount/GrossApproval) %>%
  # Remove unecessary variables 
  select(-one_of(cols_to_drop)) 

# Convert probabilities object to data frame 
default_probs = 
  data_frame(one_year_prob = p_1 %>% unlist(),
             five_year_prob = p_5 %>% unlist()) 
```

##Pre-processing

We pre-process the data by centering and scaling all continuous features. 
We apply the same normalization from the training data with only defaulted loans
to the portfolio data used for prediction in the loss at default model. 
Similarly, we apply the same training normalization from the entire training 
set to the portfolio data used for prediction in the default probability model.

```{r}
# Define pre-processing steps
preProcessSteps = c("nzv")
```

#Feature Selection

To select the features that are used in the loss at default model, 
we perform recursive feature elimination. 

```{r, eval = FALSE}
# Set the recursive feature elimination parameters 
set.seed(1234)
rfe.cntrl = rfeControl(functions = rfFuncs,
                      method = "cv",
                      number = 5)
train.cntrl = trainControl(selectionFunction = "oneSE")

# Perform recursive feature elimination to select variables
rfe.results =
  rfe(percent_loss~.,
      data = train_with_defaults,
      rfeControl = rfe.cntrl,
      preProc = preProcessSteps,
      # sizes =  seq(12,132,10), # commented out to reduce runtime 
      metric = "ROC",
      trControl = train.cntrl)

# Map factor levels back to their respective features 
selected_vars <- map(predictors(rfe.results), 
                      ~str_match(.x, names(train_with_defaults))) %>% 
  unlist() %>% 
  .[!is.na(.)] %>%
  unique()

# Create data frame with these selected variables 
train_selected_vars <- train_with_defaults %>%
  select(one_of(selected_vars), percent_loss)
```

#Model Fitting 

To tune hyperparameters, we use 5-fold cross-validation with the "one standard 
error" rule. 

```{r}
# Define cross-validation controls 
cvCtrl = trainControl(method = "cv", 
                       number = 5,
                       selectionFunction = "oneSE",
                       classProbs = TRUE)
                       # allowParallel = TRUE)
```

##Random Forest Model 
```{r}
# Define grid of tuning parameters
rfGrid = expand.grid(.mtry = c(2, 4, 6, 8, 10, 14))
# Fit random forest model
set.seed(1234)
rf.fit = train(percent_loss ~ .,
                   data = train_selected_vars,
                   preProc = preProcessSteps,
                   method = "rf",
                   # tuneGrid = rfGrid, # commented out to reduce runtime 
                   trControl = cvCtrl)
```

#Model Evaluation 

##Test Set Prediction  

We calculate the expected loss and default probability of each loan in the 
portfolio of 500 loans by using the model of expected loss and best model
of default probability. 

```{r}
# Make data frame with predicted loss and default probability for each loan
prediction_df = 
  bind_cols(
    # Predicted losses from loss at default model 
    data_frame(default_loss = predict(rf.fit, portfolio)) %>%
      # Apply sigmoid transformation to recover values between 0-1 
      mutate(default_loss = 1 / (1 + exp(1)^(-default_loss))),
    # Bind 1- and 5-year default probabilities
    default_probs) 
 
# USE THIS CODE IF LOSS AT DEFAUL MODEL PREDICTS DOLLAR AMOUNTS
  # ## Renormalize loss amounts into dollars 
  # mutate(default_loss = sd_GrossChargeOffAmount * default_loss +
  #          mean_GrossChargeOffAmount) %>%
  # ## Ensure loss is zero or above 
  # mutate(default_loss = ifelse(default_loss < 0, 0, default_loss))
```

##Simulate Distribution of Total Loss 

```{r}
# Initialize simulation parameters 
num_simulations = 1000
num_loans = nrow(prediction_df)
one_year_default_result <- vector("numeric", num_simulations)
five_year_default_result <- vector("numeric", num_simulations)

# run through simulations for the 500 loans
for (simulation in c(1:num_simulations)) {
  one_year_loss = 0  
  five_year_loss = 0  
  
  # generate a uniform random variable and comparing that to our prediction 
  uniform_probs <- runif(num_loans)
  for (loan in c(1:num_loans)) {
    
     # if the uniform rv is smaller than one year default prob, loan has defaulted
    if (uniform_probs[loan] < prediction_df$one_year_prob[loan]) {
      one_year_loss = one_year_loss + prediction_df$default_loss[loan]
    } 
    
     # if the uniform rv is smaller than five year default prob, loan has defaulted
    if (uniform_probs[loan] < prediction_df$five_year_prob[loan]) {
      five_year_loss = five_year_loss + prediction_df$default_loss[loan]
    } 
  }
  
  # Update result vectors with total loss of current simulation 
  one_year_default_result[simulation] = one_year_loss
  five_year_default_result[simulation] = five_year_loss
}

# Convert result objects to data frames 
one_year_default_result = data.frame(one_year_default_result)
five_year_default_result = data.frame(five_year_default_result)
```

##Plot Expected Loss Distributions
```{r}
# Plot the histogram for the 1-year losses 
ggplot(data = one_year_default_result, aes(one_year_default_result)) +
  geom_histogram(position = "identity") +
  labs(x = "Total Loss at Default from 500 Loans", y = "Count", 
       title = "One-Year Expected Loss Distribution from 500 Loan Portfolio")
```

```{r}
# Plot the histogram for the 5-year losses 
ggplot(data = five_year_default_result, aes(five_year_default_result)) +
  geom_histogram(position = "identity") +
  labs(x = "Total Loss at Default from 500 Loans", y = "Count", 
       title = "Five-Year Expected Loss Distribution from 500 Loan Portfolio")
```

##Compute Value-at-Risk

TO DO:

- Fix total loss distribution, since it represents the *sum* of loss percentages 
- Measure the risk in terms of the VaR at the 95% and 99% levels 
(include confidence bands for your estimates). 
- State the loan selection method --> Ask Theo 

##Compute Average Value-at-Risk

- Measure the risk in terms of the Average VaR at the 95% and 99% levels 
(include confidence bands for your estimates). 

```{r, eval = FALSE, echo = FALSE}
# COMMENTED OUT BECAUSE NOT USING THIS NOW 

# We fit an elastic net model as follows:
# # # Define grid of tuning parameters
# elasticGrid = expand.grid(.alpha = seq(0, 1, 0.1),
#                          .lambda = seq(0, 0.05, by = 0.005))
# 
# # Fit penalized linear regression model (elastic net)
# set.seed(1234)
# elastic.fit = train(percent_loss ~ .,
#                    data = train_selected_vars,
#                    preProc = preProcessSteps,
#                    method = "glmnet",
#                    # tuneGrid = elasticGrid, 
#                    trControl = cvCtrl)

# preProcessSteps = c("center", "scale", "nzv")

# # Apply pre-processing steps from training set with only defaults
# preProcessObject_LAD = preProcess(train_with_defaults, method = preProcessSteps)
# train_with_defaults = predict(preProcessObject_LAD, train_with_defaults)
# 
# # Apply same pre-processing to portfolio for loss at default model
# portfolio_LAD = predict(preProcessObject_LAD, portfolio)

# # Apply pre-processing steps from training set with all data 
# preProcessObject_probs = preProcess(train, method = preProcessSteps)
# train = predict(preProcessObject_probs, train)
# 
# # Apply same pre-processing to portfolio for default probability model 
# portfolio_probs = predict(preProcessObject_probs, portfolio)

# We extract the `GrossChargeOffAmount` mean and standard deviation for 
# later re-normalization. 
# Extract GrossChargeOffAmount mean and standard deviation
# mean_GrossChargeOffAmount = mean(train_with_defaults$GrossChargeOffAmount)
# sd_GrossChargeOffAmount = sd(train_with_defaults$GrossChargeOffAmount)
# median_GrossChargeOffAmount = median(train_with_defaults$GrossChargeOffAmount)

# Histogram of percentage loss 
# train_with_defaults %>%
#   ggplot(mapping = aes(x = percent_loss)) +
#   geom_histogram(binwidth = 0.01) 
#   scale_x_continuous(labels = scales::dollar)

# OTHER NOTES: 
# response = grosschargeoffamount/grossApprovalAmount
# apply transformation on predicted LAD values from portfolio 
# apply sigmoid function
```
