---
title: "MSE 246 Data Cleaning"
author: "Samuel Hansen"
date: "1/21/2017"
output: 
  html_document:
    toc: true
    keep_md: true 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, 
                      message = FALSE, cache = TRUE, eval = FALSE)
```

#Overview

This script builds predictive models of small-business defaults using 
data spanning 1990-2014 provided by the Small Business Association (SBA). 
To do so, this script implements a pipeline that:

1. Performs feature engineering;
2. Splits the data into train and test sets;
3. Normalizes continuous features;
4. Selects features using recursive feature elimination;
5. Trains binary outcome predictive models, including LASSO and random 
forests. 

Lastly, we evaluate the performance of these models on the held-out test set
in terms of AUC, sensitivity, and calibration. 

```{r, eval = TRUE}
# Initialize libraries 
library(ggrepel)
library(knitr)
library(lubridate)
library(caret)
library(stringr)
library(plotROC)
library(pROC)
library(tidyverse)

# Initialize input files 
train_file_in <- "../data/train.rds"
test_file_in <- "../data/test.rds"

# Read in data 
train <- read_rds(train_file_in)
test <- read_rds(test_file_in)
```

#Feature Engineering

We engineered the following features from the raw data: 

- `first_zip_digit`: the first digit of the borrower's zip code;
- `NAICS`: the first two digits of the NAICS code;
- `subpgmdesc`: condensed infrequent factor levels into "other" category;
- `approval_year`: extracted year from loan approval datetime object.



#Data Preprocessing

Some variables are on different scales; for example, `Gross Approval` 
varies in dollar amounts from \$30,000  to \$4,000,000, whereas 
`Term in Months` ranges from 1 to 389. In turn, we center and scale the 
predictors to apply regularization techniques during the modeling phase. 

```{r}
# Remove unnecesary features for modeling 
train <- train %>% select(-c(GrossChargeOffAmount, ChargeOffDate))
test <- test %>% select(-c(GrossChargeOffAmount, ChargeOffDate))
```

```{r}
# Define pre-processing steps to apply to training data
# preProcessSteps <- c("center", "scale") "pca"? 
preProcessSteps <- c("center", "scale", "nzv")

# Apply same pre-processing steps to the test set
preProcessObject <- preProcess(train, method = preProcessSteps)
train <- predict(preProcessObject, train)
test <- predict(preProcessObject, test)
```

#Feature Selection

We perform feature selection using recursive feature elimination
with 10-fold cross-validation. This method uses the 
`rfFuncs` parameter, which uses random forests to remove 
variables with low variable importance.
```{r, eval=FALSE}
# Set the recursive feature elimination parameters 
set.seed(1234)
rfFuncs$summary <- twoClassSummary
rfe.cntrl <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      number = 5,
                      returnResamp = "final")
train.cntrl <- trainControl(selectionFunction = "oneSE",
                            classProbs = TRUE,
                            summaryFunction = twoClassSummary)

# Perform recursive feature elimination to select variables
rfe.results <-
  rfe(LoanStatus~.,
      data = train,
      rfeControl = rfe.cntrl,
      preProc = preProcessSteps,
      tuneLength = 10,
      metric = "ROC",
      trControl = train.cntrl)
```

```{r, echo = FALSE, eval=TRUE}
rfe.results <- read_rds("../models/rfe.results.rds")
```

The following table shows that recursive feature selection 
chooses `r rfe.results[["results"]]$Variables[which.max(rfe.results[["results"]]$ROC)]`
variables to include in subsequent model building.
```{r}
print(rfe.results)
```

The procedure selects `r rfe.results[["results"]]$Variables[which.max(rfe.results[["results"]]$ROC)]` 
variables because AUC is maximized (see plot below):
```{r}
ggplot(rfe.results) +
  labs(x = "Number of Variables",
       y = "AUC (Cross-Validated)",
       title = "Recursive Feature Elimination\nNumber of Variables vs. AUC")
```

The importances of the top 30 selected features are given by:
```{r, fig.height=8, fig.width=7}
data_frame(predictor = rownames(varImp(rfe.results)), 
           var_imp = varImp(rfe.results)$Overall) %>%
  slice(1:30) %>%
  ggplot(mapping = aes(x = reorder(predictor, var_imp), y = var_imp)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "", 
       y = "Variable Importance", 
       title = "Recursive Feature Elimination Variable Importance")
```

```{r}
# Map factor levels back to their respective features 
(selected_vars <- map(predictors(rfe.results), ~str_match(.x, names(train))) %>% 
  unlist() %>% 
  .[!is.na(.)] %>%
  unique())
train_selected_vars <- train %>%
  select(one_of(selected_vars), LoanStatus)
```

#Model Fitting 

Using these selected features, we fit models predicting the binary outcome
of whether a small busniess defaults on a loan. To tune hyperparameters,
we use 10-fold cross-validation with the one standard-error rule, which selects
parameters that obtain the highest cross-validated AUC within one standard error
of the maximum. 

```{r}
# Define cross-validation controls 
cvCtrl <- trainControl(method = "cv", 
                       number = 10,
                       summaryFunction = twoClassSummary, 
                       selectionFunction = "oneSE",
                       classProbs = TRUE)
```

##Elastic Net

We fit an elastic net model as follows:
```{r, eval=FALSE}
# Fit penalized logistic regression model (elastic net)
elastic.fit <- train(LoanStatus ~ .,
                   data = train_selected_vars,
                   preProc = preProcessSteps,
                   method = "glmnet",
                   tuneLength = 10,
                   family = "binomial",
                   trControl = cvCtrl,
                   metric = "ROC")
```

The elastic net model was selected with the following hyperparameters:
```{r, echo = FALSE, fig.width=6, fig.height=6}
elastic.fit <- read_rds("../models/elastic.fit.rds")
elastic.fit
plot(elastic.fit)
```

##Random Forest

We fit a random forest model as follows:
```{r, eval=FALSE}
# Fit penalized logistic regression model (elastic net)
rf.fit <- train(LoanStatus ~ .,
                   data = train_selected_vars,
                   preProc = preProcessSteps,
                   method = "rf",
                   tuneLength = 10,
                   trControl = cvCtrl,
                   metric = "ROC")
```

The random forestmodel was selected with the following hyperparameters:
```{r, echo = FALSE, fig.width=6, fig.height=6}
rf.fit <- read_rds("../models/rf.fit.rds")
rf.fit
plot(rf.fit)
```

#Model Evaluation 

##In-Sample Evaluation 

###Training AUC and Sensitivity of Best Models 

The following plot compares averaged **training** area under the ROC curve 
and sensitivity across the model types with optimized parameters.

```{r}
# Evaluate performance of each model on training data
bind_rows(getTrainPerf(elastic.fit), 
          getTrainPerf(rf.fit)) %>% 
  as_data_frame() %>%
  mutate(method = recode(method, 
                         "glmnet" = "Elastic Net", "rf" = "Random Forest")) %>%
  ggplot(mapping = aes(x = TrainSens, y = TrainROC, label = method)) +
  geom_point() +
  geom_text_repel() +
  labs(x = "Train Sensitivity", y = "Train AUC", 
       title = "Training Sensitivity vs. AUC by Model Type")
```

###Distribution of Resampled Training AUC, Sensitivity, and Specificity 

To examine the spread of **training** area under the ROC curve,
sensitivity, and specificity across model types, we leverage the resampled
data generated during the cross-valiation of modeling fitting to plot 
their respective distributions.  

```{r}
# Generate resamples from data 
resamps <- resamples(list(Elastic_Net = elastic.fit,
                          Random_Forest = rf.fit))

# Generate boxplots 
metric_labs = c("ROC" = "AUC", "Sens" = "Sensitivity", "Spec" = "Specificity")
resamps$values %>%
  gather(method, value, `Elastic_Net~ROC`:`Random_Forest~Spec`) %>%
  separate(method, c("method", "metric"), sep = "~", remove = TRUE) %>%
  ggplot(mapping = aes(x = method, y = value)) +
  geom_boxplot() +
  facet_wrap(~metric, scales = "free_y", 
             labeller = labeller(metric = metric_labs)) +
  labs(x = "Model Type", y = "Metric Value", 
       title = "Spread of Training AUC, Sensitivity, and Specificity")

```

###Training ROC Curves 

Lastly, we can examine the training ROC curves by model type. 

```{r}
# Evaluate performance of trained models on training set 
trainResults <- data.frame(true_value = train$LoanStatus)
trainResults$randomForest <- predict(rf.fit, train, type = "prob")[,"default"]
trainResults$elasticNet <- predict(elastic.fit, train, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = trainResults$randomForest,
                       response = trainResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = trainResults$elasticNet,
                       response = trainResults$true_value))) %>%
  gather(method, auc_value, randomForest:elasticNet) %>%
  mutate(auc_label = paste("AUC =", round(auc_value,3)))

# Gather results in long format 
trainResults <- 
  trainResults %>%
  gather(method, predicted_prob, randomForest:elasticNet) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))
```

##Out-of-Sample Evaluation 

###Test ROC Curves 

```{r}
# Evaluate performance of trained models on test set 
testResults <- data.frame(true_value = test$LoanStatus)
testResults$randomForest <- predict(rf.fit, test, type = "prob")[,"default"]
testResults$elasticNet <- predict(elastic.fit, test, type = "prob")[,"default"]

# Compute AUC by model type 
aucs <-
  data_frame(randomForest = pROC::auc(roc(predictor = testResults$randomForest,
                       response = testResults$true_value)),
             elasticNet = pROC::auc(roc(predictor = testResults$elasticNet,
                       response = testResults$true_value))) %>%
  gather(method, auc_value, randomForest:elasticNet) %>%
  mutate(auc_label = paste("AUC =", round(auc_value,3)))

# Gather results in long format 
testResults <- 
  testResults %>%
  gather(method, predicted_prob, randomForest:elasticNet) %>%
  mutate(true_value = ifelse(true_value == "default", 1, 0))
```

```{r, fig.width=9, fig.height=6}
# Plot ROC curves by model type 
model_labels <- c("randomForest" = "Random Forest",
                  "elasticNet" = "Elastic Net")
testResults %>%
  ggplot(mapping = aes(d = true_value, m = predicted_prob)) +
  geom_roc(n.cuts = 5, labelsize = 2, labelround = 3) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  labs(x = "False Positive Fraction", y = "True Positive Fraction",
       title = "ROC Curves by Model Type") +
  facet_wrap(~method, labeller = labeller(method = model_labels)) +
  geom_text(data = aucs, aes(x = 0.75, y = 0.5, label = auc_label), 
                    colour = "black", inherit.aes = FALSE, parse = FALSE)
```

###Test Calibration Plots

The following calibration plots depict the extent to which our models' 
predicted probabilities of default align with the actual probabilities of
default. 

```{r, fig.width=9, fig.height=6}
# Make calibration plots, facetted by model type
pred_prob_midpoints <- data_frame(midpoint = rep(seq(0.025, 0.975, 0.05), each = 2))
testResults %>%
  mutate(prob_bin = cut_width(predicted_prob,width = 0.05)) %>%
  group_by(prob_bin, method) %>%
  summarise(prob_default = mean(true_value, na.rm = TRUE),
            n = n()) %>%
  bind_cols(., pred_prob_midpoints) %>%
  ungroup() %>%
  ggplot(mapping = aes(x = midpoint, y = prob_default, label = n)) +
  geom_line() +
  geom_point(mapping = aes(size = n)) +
  geom_text_repel(mapping = aes(color = "red")) +
  annotate(geom = "segment", x = 0, xend = 1, y = 0, yend = 1,
           color = "black", linetype = 2) +
  scale_x_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_y_continuous(labels = scales::percent,
                     breaks = seq(0, 1, by = 0.1)) +
  scale_colour_discrete(guide = FALSE) +
  scale_size(name = "Number of\nPredictions",
             labels = scales::comma) +
  labs(x = "Predicted Default Probability (Bin Midpoint)",
       y = "Actual Default Probability",
       title = "Calibration Plot: Predicted vs. Actual Default Probability") +
  facet_wrap(~method, labeller = labeller(method = model_labels))
```

```{r, echo = FALSE}
#IGNORE THIS CODE FOR NOW

# # Selection by Filter
# # Set the selection by filter parameters 
# set.seed(1234)
# rfFuncs$summary <- twoClassSummary
# sbf.cntrl <- sbfControl(functions = rfSBF,
#                       method = "cv",
#                       number = 5,
#                       returnResamp = "final")
# train.cntrl <- trainControl(selectionFunction = "oneSE",
#                             classProbs = TRUE,
#                             summaryFunction = twoClassSummary)
# 
# # Perform selection by filter to select variables 
# sbf.results <- 
#   sbf(LoanStatus~., 
#       data = train,
#       sbfControl = sbf.cntrl, 
#       preProc = preProcessSteps,
#       metric = "ROC",
#       trControl = train.cntrl)
```

